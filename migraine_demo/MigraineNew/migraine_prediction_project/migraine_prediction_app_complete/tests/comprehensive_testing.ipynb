{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Testing of Migraine Prediction Model\n",
    "\n",
    "This notebook provides comprehensive testing of the migraine prediction model, including:\n",
    "\n",
    "1. Data validation\n",
    "2. Model training and evaluation\n",
    "3. Cross-validation\n",
    "4. Performance metrics analysis\n",
    "5. Expert contribution analysis\n",
    "6. Trigger sensitivity analysis\n",
    "\n",
    "The goal is to validate that the model meets the >95% performance target specified in the PRD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import tensorflow as tf\n",
    "import pygmo as pg\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Add the project root to the path\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import our modules\n",
    "from data_generator.synthetic_data_generator import SyntheticDataGenerator\n",
    "from migraine_prediction_model import MigrainePredictionModel\n",
    "from performance_metrics import MigrainePerformanceMetrics\n",
    "from optimized_model import OptimizedMigrainePredictionModel\n",
    "\n",
    "# Configure matplotlib\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "base_dir = os.path.dirname(os.path.abspath('.'))\n",
    "data_dir = os.path.join(base_dir, 'test_data')\n",
    "output_dir = os.path.join(base_dir, 'test_output')\n",
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Data directory: {data_dir}\")\n",
    "print(f\"Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data for testing\n",
    "data_generator = SyntheticDataGenerator(\n",
    "    num_patients=200,  # 200 patients\n",
    "    days=180,          # 6 months of data\n",
    "    output_dir=data_dir,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Generate data if it doesn't exist\n",
    "if not os.path.exists(os.path.join(data_dir, 'combined_data.csv')):\n",
    "    print(\"Generating synthetic data...\")\n",
    "    data_generator.generate_data()\n",
    "    print(\"Data generation complete.\")\n",
    "else:\n",
    "    print(\"Using existing synthetic data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and examine the data\n",
    "combined_data = pd.read_csv(os.path.join(data_dir, 'combined_data.csv'))\n",
    "sleep_data = pd.read_csv(os.path.join(data_dir, 'sleep_data.csv'))\n",
    "weather_data = pd.read_csv(os.path.join(data_dir, 'weather_data.csv'))\n",
    "stress_diet_data = pd.read_csv(os.path.join(data_dir, 'stress_diet_data.csv'))\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Combined data shape: {combined_data.shape}\")\n",
    "print(f\"Sleep data shape: {sleep_data.shape}\")\n",
    "print(f\"Weather data shape: {weather_data.shape}\")\n",
    "print(f\"Stress/Diet data shape: {stress_diet_data.shape}\")\n",
    "\n",
    "# Display first few rows of combined data\n",
    "combined_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values in combined data:\")\n",
    "print(combined_data.isnull().sum())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\nData types in combined data:\")\n",
    "print(combined_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable distribution\n",
    "migraine_count = combined_data['next_day_migraine'].value_counts()\n",
    "migraine_percentage = combined_data['next_day_migraine'].mean() * 100\n",
    "\n",
    "print(f\"Migraine distribution:\\n{migraine_count}\")\n",
    "print(f\"Migraine percentage: {migraine_percentage:.2f}%\")\n",
    "\n",
    "# Plot migraine distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='next_day_migraine', data=combined_data)\n",
    "plt.title('Distribution of Migraine Events')\n",
    "plt.xlabel('Next Day Migraine')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['No Migraine', 'Migraine'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze patient distribution\n",
    "patient_counts = combined_data['patient_id'].value_counts()\n",
    "print(f\"Number of unique patients: {len(patient_counts)}\")\n",
    "print(f\"Average days per patient: {patient_counts.mean():.2f}\")\n",
    "\n",
    "# Plot patient distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "patient_counts.hist(bins=30)\n",
    "plt.title('Distribution of Days per Patient')\n",
    "plt.xlabel('Number of Days')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze migraine frequency by patient\n",
    "patient_migraine = combined_data.groupby('patient_id')['next_day_migraine'].mean() * 100\n",
    "chronic_patients = (patient_migraine >= 15).mean() * 100  # ≥15 days/month = ≥15% of days\n",
    "\n",
    "print(f\"Percentage of chronic patients (≥15 migraine days/month): {chronic_patients:.2f}%\")\n",
    "print(f\"Percentage of episodic patients (<15 migraine days/month): {100 - chronic_patients:.2f}%\")\n",
    "\n",
    "# Plot migraine frequency distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "patient_migraine.hist(bins=30)\n",
    "plt.axvline(x=15, color='red', linestyle='--', label='Chronic Threshold (15%)')\n",
    "plt.title('Distribution of Migraine Frequency by Patient')\n",
    "plt.xlabel('Percentage of Days with Migraine')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Validate Migraine Triggers\n",
    "\n",
    "Let's check if the synthetic data correctly implements the specified migraine triggers:\n",
    "- Barometric pressure drops ≥ 5 hPa within 24h\n",
    "- Sleep disruptions (< 5h or > 9h sleep)\n",
    "- Stress spikes (sudden +3-5 points on a 1-10 scale)\n",
    "- Dietary triggers (alcohol, caffeine, chocolate intake)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check barometric pressure drops\n",
    "pressure_drop = combined_data['pressure_change_24h'] <= -5\n",
    "migraine_with_pressure_drop = combined_data.loc[pressure_drop, 'next_day_migraine'].mean() * 100\n",
    "migraine_without_pressure_drop = combined_data.loc[~pressure_drop, 'next_day_migraine'].mean() * 100\n",
    "\n",
    "print(\"Barometric Pressure Drop Analysis:\")\n",
    "print(f\"Percentage of days with pressure drop ≥ 5 hPa: {pressure_drop.mean() * 100:.2f}%\")\n",
    "print(f\"Migraine percentage with pressure drop: {migraine_with_pressure_drop:.2f}%\")\n",
    "print(f\"Migraine percentage without pressure drop: {migraine_without_pressure_drop:.2f}%\")\n",
    "print(f\"Difference: {migraine_with_pressure_drop - migraine_without_pressure_drop:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "data = pd.DataFrame({\n",
    "    'Pressure Drop': ['Yes', 'No'],\n",
    "    'Migraine Percentage': [migraine_with_pressure_drop, migraine_without_pressure_drop]\n",
    "})\n",
    "sns.barplot(x='Pressure Drop', y='Migraine Percentage', data=data)\n",
    "plt.title('Migraine Percentage by Barometric Pressure Drop')\n",
    "plt.ylabel('Migraine Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check sleep disruptions\n",
    "sleep_disruption = (combined_data['total_sleep_hours'] < 5) | (combined_data['total_sleep_hours'] > 9)\n",
    "migraine_with_sleep_disruption = combined_data.loc[sleep_disruption, 'next_day_migraine'].mean() * 100\n",
    "migraine_without_sleep_disruption = combined_data.loc[~sleep_disruption, 'next_day_migraine'].mean() * 100\n",
    "\n",
    "print(\"Sleep Disruption Analysis:\")\n",
    "print(f\"Percentage of days with sleep disruption (< 5h or > 9h): {sleep_disruption.mean() * 100:.2f}%\")\n",
    "print(f\"Migraine percentage with sleep disruption: {migraine_with_sleep_disruption:.2f}%\")\n",
    "print(f\"Migraine percentage without sleep disruption: {migraine_without_sleep_disruption:.2f}%\")\n",
    "print(f\"Difference: {migraine_with_sleep_disruption - migraine_without_sleep_disruption:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "data = pd.DataFrame({\n",
    "    'Sleep Disruption': ['Yes', 'No'],\n",
    "    'Migraine Percentage': [migraine_with_sleep_disruption, migraine_without_sleep_disruption]\n",
    "})\n",
    "sns.barplot(x='Sleep Disruption', y='Migraine Percentage', data=data)\n",
    "plt.title('Migraine Percentage by Sleep Disruption')\n",
    "plt.ylabel('Migraine Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check stress spikes\n",
    "stress_spike = combined_data['stress_level'] >= 7  # High stress (7-10 on scale)\n",
    "migraine_with_stress_spike = combined_data.loc[stress_spike, 'next_day_migraine'].mean() * 100\n",
    "migraine_without_stress_spike = combined_data.loc[~stress_spike, 'next_day_migraine'].mean() * 100\n",
    "\n",
    "print(\"Stress Spike Analysis:\")\n",
    "print(f\"Percentage of days with high stress (≥ 7): {stress_spike.mean() * 100:.2f}%\")\n",
    "print(f\"Migraine percentage with high stress: {migraine_with_stress_spike:.2f}%\")\n",
    "print(f\"Migraine percentage without high stress: {migraine_without_stress_spike:.2f}%\")\n",
    "print(f\"Difference: {migraine_with_stress_spike - migraine_without_stress_spike:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "data = pd.DataFrame({\n",
    "    'High Stress': ['Yes', 'No'],\n",
    "    'Migraine Percentage': [migraine_with_stress_spike, migraine_without_stress_spike]\n",
    "})\n",
    "sns.barplot(x='High Stress', y='Migraine Percentage', data=data)\n",
    "plt.title('Migraine Percentage by Stress Level')\n",
    "plt.ylabel('Migraine Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dietary triggers\n",
    "dietary_trigger = (combined_data['alcohol_consumed'] > 0) | \\\n",
    "                  (combined_data['caffeine_consumed'] > 0) | \\\n",
    "                  (combined_data['chocolate_consumed'] > 0)\n",
    "migraine_with_dietary_trigger = combined_data.loc[dietary_trigger, 'next_day_migraine'].mean() * 100\n",
    "migraine_without_dietary_trigger = combined_data.loc[~dietary_trigger, 'next_day_migraine'].mean() * 100\n",
    "\n",
    "print(\"Dietary Trigger Analysis:\")\n",
    "print(f\"Percentage of days with dietary triggers: {dietary_trigger.mean() * 100:.2f}%\")\n",
    "print(f\"Migraine percentage with dietary triggers: {migraine_with_dietary_trigger:.2f}%\")\n",
    "print(f\"Migraine percentage without dietary triggers: {migraine_without_dietary_trigger:.2f}%\")\n",
    "print(f\"Difference: {migraine_with_dietary_trigger - migraine_without_dietary_trigger:.2f}%\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "data = pd.DataFrame({\n",
    "    'Dietary Trigger': ['Yes', 'No'],\n",
    "    'Migraine Percentage': [migraine_with_dietary_trigger, migraine_without_dietary_trigger]\n",
    "})\n",
    "sns.barplot(x='Dietary Trigger', y='Migraine Percentage', data=data)\n",
    "plt.title('Migraine Percentage by Dietary Triggers')\n",
    "plt.ylabel('Migraine Percentage (%)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze combined triggers\n",
    "trigger_count = pressure_drop.astype(int) + sleep_disruption.astype(int) + \\\n",
    "               stress_spike.astype(int) + dietary_trigger.astype(int)\n",
    "\n",
    "# Calculate migraine percentage by number of triggers\n",
    "migraine_by_triggers = combined_data.groupby(trigger_count)['next_day_migraine'].mean() * 100\n",
    "\n",
    "print(\"Combined Triggers Analysis:\")\n",
    "print(f\"Migraine percentage by number of triggers:\\n{migraine_by_triggers}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "migraine_by_triggers.plot(kind='bar')\n",
    "plt.title('Migraine Percentage by Number of Triggers')\n",
    "plt.xlabel('Number of Triggers')\n",
    "plt.ylabel('Migraine Percentage (%)')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create optimized model with reduced settings for testing\n",
    "optimizer = OptimizedMigrainePredictionModel(\n",
    "    data_dir=data_dir,\n",
    "    output_dir=output_dir,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Modify config for faster testing\n",
    "optimizer.config['epochs'] = 20\n",
    "optimizer.config['expert_pop_size'] = 10\n",
    "optimizer.config['expert_generations'] = 3\n",
    "optimizer.config['gating_pop_size'] = 10\n",
    "optimizer.config['gating_generations'] = 3\n",
    "optimizer.config['e2e_pop_size'] = 10\n",
    "optimizer.config['e2e_generations'] = 3\n",
    "\n",
    "# Display the configuration\n",
    "print(\"Model Configuration:\")\n",
    "for key, value in optimizer.config.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train the model\n",
    "print(\"Building and training the model...\")\n",
    "model, history, test_metrics = optimizer.train_optimized_model()\n",
    "\n",
    "# Print test metrics\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(f\"AUC: {test_metrics['roc_auc']:.4f} (Target: {optimizer.metrics_tracker.config['target_auc']})\")\n",
    "print(f\"F1 Score: {test_metrics['f1_score']:.4f} (Target: {optimizer.metrics_tracker.config['target_f1']})\")\n",
    "print(f\"High-Risk Sensitivity: {test_metrics['high_risk_sensitivity']:.4f} (Target: {optimizer.metrics_tracker.config['target_sensitivity']})\")\n",
    "print(f\"Inference Time: {test_metrics['inference_time_ms']:.2f} ms (Target: {optimizer.metrics_tracker.config['target_latency_ms']} ms)\")\n",
    "print(f\"Overall Performance Score: {test_metrics['performance_score']:.1f}%\")\n",
    "print(f\"Target Met: {'Yes' if test_metrics['overall_target_met'] else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "# Plot AUC\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(history.history['auc'], label='Train')\n",
    "plt.plot(history.history['val_auc'], label='Validation')\n",
    "plt.axhline(y=optimizer.metrics_tracker.config['target_auc'], color='r', linestyle='--', label='Target')\n",
    "plt.title('AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(history.history['f1_score'], label='Train')\n",
    "plt.plot(history.history['val_f1_score'], label='Validation')\n",
    "plt.axhline(y=optimizer.metrics_tracker.config['target_f1'], color='r', linestyle='--', label='Target')\n",
    "plt.title('F1 Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(history.history['loss'], label='Train')\n",
    "plt.plot(history.history['val_loss'], label='Validation')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Standard threshold confusion matrix\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(test_metrics['confusion_matrix'], annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['No Migraine', 'Migraine'],\n",
    "            yticklabels=['No Migraine', 'Migraine'])\n",
    "plt.title('Confusion Matrix (Standard Threshold)')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "# High-risk threshold confusion matrix\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(test_metrics['high_risk_confusion_matrix'], annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=['No Migraine', 'Migraine'],\n",
    "            yticklabels=['No Migraine', 'Migraine'])\n",
    "plt.title(f'Confusion Matrix (High-Risk Threshold: {optimizer.metrics_tracker.config[\"high_risk_threshold\"]:.2f})')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(test_metrics['fpr'], test_metrics['tpr'], 'b-', linewidth=2,\n",
    "         label=f'ROC curve (AUC = {test_metrics[\"roc_auc\"]:.3f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=2)\n",
    "plt.axhline(y=optimizer.metrics_tracker.config['target_sensitivity'], color='r', linestyle=':', \n",
    "            label=f'Target Sensitivity = {optimizer.metrics_tracker.config[\"target_sensitivity\"]}')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation\n",
    "def perform_cross_validation(data_dir, n_splits=5, seed=42):\n",
    "    # Load data\n",
    "    combined_data = pd.read_csv(os.path.join(data_dir, 'combined_data.csv'))\n",
    "    \n",
    "    # Get unique patient IDs\n",
    "    patient_ids = combined_data['patient_id'].unique()\n",
    "    \n",
    "    # Initialize KFold\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Initialize results storage\n",
    "    cv_results = {\n",
    "        'auc': [],\n",
    "        'f1_score': [],\n",
    "        'high_risk_sensitivity': [],\n",
    "        'performance_score': []\n",
    "    }\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf.split(patient_ids)):\n",
    "        print(f\"\\nFold {fold+1}/{n_splits}\")\n",
    "        \n",
    "        # Split patients\n",
    "        train_patients = patient_ids[train_idx]\n",
    "        test_patients = patient_ids[test_idx]\n",
    "        \n",
    "        # Create fold directory\n",
    "        fold_dir = os.path.join(output_dir, f'fold_{fold+1}')\n",
    "        os.makedirs(fold_dir, exist_ok=True)\n",
    "        \n",
    "        # Split data\n",
    "        train_data = combined_data[combined_data['patient_id'].isin(train_patients)]\n",
    "        test_data = combined_data[combined_data['patient_id'].isin(test_patients)]\n",
    "        \n",
    "        # Save split data\n",
    "        train_data.to_csv(os.path.join(fold_dir, 'train_data.csv'), index=False)\n",
    "        test_data.to_csv(os.path.join(fold_dir, 'test_data.csv'), index=False)\n",
    "        \n",
    "        # Create optimized model with reduced settings\n",
    "        optimizer = OptimizedMigrainePredictionModel(\n",
    "            data_dir=fold_dir,\n",
    "            output_dir=fold_dir,\n",
    "            seed=seed + fold\n",
    "        )\n",
    "        \n",
    "        # Modify config for faster testing\n",
    "        optimizer.config['epochs'] = 10\n",
    "        optimizer.config['expert_pop_size'] = 5\n",
    "        optimizer.config['expert_generations'] = 2\n",
    "        optimizer.config['gating_pop_size'] = 5\n",
    "        optimizer.config['gating_generations'] = 2\n",
    "        optimizer.config['e2e_pop_size'] = 5\n",
    "        optimizer.config['e2e_generations'] = 2\n",
    "        \n",
    "        # Train model\n",
    "        try:\n",
    "            model, history, test_metrics = optimizer.train_optimized_model()\n",
    "            \n",
    "            # Store results\n",
    "            cv_results['auc'].append(test_metrics['roc_auc'])\n",
    "            cv_results['f1_score'].append(test_metrics['f1_score'])\n",
    "            cv_results['high_risk_sensitivity'].append(test_metrics['high_risk_sensitivity'])\n",
    "            cv_results['performance_score'].append(test_metrics['performance_score'])\n",
    "            \n",
    "            # Print fold results\n",
    "            print(f\"Fold {fold+1} Results:\")\n",
    "            print(f\"AUC: {test_metrics['roc_auc']:.4f}\")\n",
    "            print(f\"F1 Score: {test_metrics['f1_score']:.4f}\")\n",
    "            print(f\"High-Risk Sensitivity: {test_metrics['high_risk_sensitivity']:.4f}\")\n",
    "            print(f\"Performance Score: {test_metrics['performance_score']:.1f}%\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fold {fold+1}: {e}\")\n",
    "    \n",
    "    # Calculate mean and std\n",
    "    cv_summary = {}\n",
    "    for metric in cv_results.keys():\n",
    "        cv_summary[f'{metric}_mean'] = np.mean(cv_results[metric])\n",
    "        cv_summary[f'{metric}_std'] = np.std(cv_results[metric])\n",
    "    \n",
    "    return cv_results, cv_summary\n",
    "\n",
    "# Run cross-validation with 3 folds for demonstration\n",
    "cv_results, cv_summary = perform_cross_validation(data_dir, n_splits=3, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display cross-validation results\n",
    "print(\"Cross-Validation Summary:\")\n",
    "print(f\"AUC: {cv_summary['auc_mean']:.4f} ± {cv_summary['auc_std']:.4f}\")\n",
    "print(f\"F1 Score: {cv_summary['f1_score_mean']:.4f} ± {cv_summary['f1_score_std']:.4f}\")\n",
    "print(f\"High-Risk Sensitivity: {cv_summary['high_risk_sensitivity_mean']:.4f} ± {cv_summary['high_risk_sensitivity_std']:.4f}\")\n",
    "print(f\"Performance Score: {cv_summary['performance_score_mean']:.1f}% ± {cv_summary['performance_score_std']:.1f}%\")\n",
    "\n",
    "# Plot cross-validation results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot metrics by fold\n",
    "metrics = ['auc', 'f1_score', 'high_risk_sensitivity']\n",
    "targets = [optimizer.metrics_tracker.config['target_auc'], \n",
    "           optimizer.metrics_tracker.config['target_f1'], \n",
    "           optimizer.metrics_tracker.config['target_sensitivity']]\n",
    "colors = ['blue', 'green', 'red']\n",
    "\n",
    "for i, (metric, target, color) in enumerate(zip(metrics, targets, colors)):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    plt.bar(range(1, len(cv_results[metric])+1), cv_results[metric], color=color, alpha=0.7)\n",
    "    plt.axhline(y=target, color='r', linestyle='--', label=f'Target')\n",
    "    plt.axhline(y=cv_summary[f'{metric}_mean'], color='k', linestyle='-', label=f'Mean')\n",
    "    plt.title(f'{metric.replace(\"_\", \" \").title()}')\n",
    "    plt.xlabel('Fold')\n",
    "    plt.ylabel('Value')\n",
    "    plt.ylim([0, 1.05])\n",
    "    plt.xticks(range(1, len(cv_results[metric])+1))\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot performance score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, len(cv_results['performance_score'])+1), cv_results['performance_score'], color='purple', alpha=0.7)\n",
    "plt.axhline(y=95, color='r', linestyle='--', label='Target (95%)')\n",
    "plt.axhline(y=cv_summary['performance_score_mean'], color='k', linestyle='-', label='Mean')\n",
    "plt.title('Performance Score by Fold')\n",
    "plt.xlabel('Fold')\n",
    "plt.ylabel('Performance Score (%)')\n",
    "plt.ylim([0, 105])\n",
    "plt.xticks(range(1, len(cv_results['performance_score'])+1))\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Expert Contribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data for expert contribution analysis\n",
    "X_train_list, y_train, X_val_list, y_val, X_test_list, y_test = optimizer.model.load_data()\n",
    "\n",
    "# Get predictions and gate outputs\n",
    "predictions, gate_outputs = optimizer.model.model(X_test_list, training=False)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "gate_weights = gate_outputs.numpy()\n",
    "predictions = predictions.numpy()\n",
    "\n",
    "# Calculate average expert weights\n",
    "avg_expert_weights = np.mean(gate_weights, axis=0)\n",
    "expert_names = ['Sleep Expert', 'Weather Expert', 'Stress/Diet Expert']\n",
    "\n",
    "# Plot average expert weights\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(expert_names, avg_expert_weights)\n",
    "plt.title('Average Expert Contribution')\n",
    "plt.ylabel('Average Weight')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print expert contribution percentages\n",
    "expert_percentages = avg_expert_weights / np.sum(avg_expert_weights) * 100\n",
    "print(\"Expert Contribution Percentages:\")\n",
    "for name, percentage in zip(expert_names, expert_percentages):\n",
    "    print(f\"{name}: {percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze expert contribution by migraine status\n",
    "migraine_indices = np.where(y_test == 1)[0]\n",
    "no_migraine_indices = np.where(y_test == 0)[0]\n",
    "\n",
    "migraine_weights = gate_weights[migraine_indices]\n",
    "no_migraine_weights = gate_weights[no_migraine_indices]\n",
    "\n",
    "avg_migraine_weights = np.mean(migraine_weights, axis=0)\n",
    "avg_no_migraine_weights = np.mean(no_migraine_weights, axis=0)\n",
    "\n",
    "# Plot expert weights by migraine status\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(expert_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, avg_migraine_weights, width, label='Migraine Days')\n",
    "plt.bar(x + width/2, avg_no_migraine_weights, width, label='Non-Migraine Days')\n",
    "\n",
    "plt.title('Expert Contribution by Migraine Status')\n",
    "plt.ylabel('Average Weight')\n",
    "plt.xticks(x, expert_names)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print expert contribution differences\n",
    "print(\"Expert Contribution Differences (Migraine vs. Non-Migraine Days):\")\n",
    "for i, name in enumerate(expert_names):\n",
    "    diff = avg_migraine_weights[i] - avg_no_migraine_weights[i]\n",
    "    print(f\"{name}: {diff:.4f} ({'+' if diff > 0 else ''}{diff/avg_no_migraine_weights[i]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze expert contribution by prediction confidence\n",
    "high_conf_indices = np.where(predictions.flatten() > 0.8)[0]  # High confidence migraine predictions\n",
    "low_conf_indices = np.where((predictions.flatten() > 0.5) & (predictions.flatten() <= 0.8))[0]  # Low confidence migraine predictions\n",
    "\n",
    "high_conf_weights = gate_weights[high_conf_indices]\n",
    "low_conf_weights = gate_weights[low_conf_indices]\n",
    "\n",
    "avg_high_conf_weights = np.mean(high_conf_weights, axis=0) if len(high_conf_weights) > 0 else np.zeros(len(expert_names))\n",
    "avg_low_conf_weights = np.mean(low_conf_weights, axis=0) if len(low_conf_weights) > 0 else np.zeros(len(expert_names))\n",
    "\n",
    "# Plot expert weights by prediction confidence\n",
    "plt.figure(figsize=(12, 6))\n",
    "x = np.arange(len(expert_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, avg_high_conf_weights, width, label='High Confidence (>0.8)')\n",
    "plt.bar(x + width/2, avg_low_conf_weights, width, label='Low Confidence (0.5-0.8)')\n",
    "\n",
    "plt.title('Expert Contribution by Prediction Confidence')\n",
    "plt.ylabel('Average Weight')\n",
    "plt.xticks(x, expert_names)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Print expert contribution differences\n",
    "print(\"Expert Contribution by Prediction Confidence:\")\n",
    "print(f\"High confidence predictions: {len(high_conf_indices)}\")\n",
    "print(f\"Low confidence predictions: {len(low_conf_indices)}\")\n",
    "print(\"\\nExpert Contribution Differences (High vs. Low Confidence):\")\n",
    "for i, name in enumerate(expert_names):\n",
    "    if len(low_conf_weights) > 0:\n",
    "        diff = avg_high_conf_weights[i] - avg_low_conf_weights[i]\n",
    "        percent_diff = diff/avg_low_conf_weights[i]*100 if avg_low_conf_weights[i] > 0 else float('inf')\n",
    "        print(f\"{name}: {diff:.4f} ({'+' if diff > 0 else ''}{percent_diff:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"{name}: N/A (no low confidence predictions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Trigger Sensitivity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to analyze model sensitivity to triggers\n",
    "def analyze_trigger_sensitivity(model, X_test_list, trigger_type, feature_idx, feature_name):\n",
    "    # Make a copy of the test data\n",
    "    X_test_modified = [X.copy() for X in X_test_list]\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_preds = model.predict(X_test_list).flatten()\n",
    "    \n",
    "    # Modify the feature based on trigger type\n",
    "    if trigger_type == 'sleep':\n",
    "        # Simulate sleep disruption (< 5 hours)\n",
    "        X_test_modified[0][:, -1, feature_idx] = -2.0  # Set to low value (standardized scale)\n",
    "    elif trigger_type == 'weather':\n",
    "        # Simulate pressure drop\n",
    "        X_test_modified[1][:, feature_idx] = -2.0  # Set to low value (standardized scale)\n",
    "    elif trigger_type == 'stress_diet':\n",
    "        # Simulate high stress or dietary trigger\n",
    "        X_test_modified[2][:, -1, feature_idx] = 2.0  # Set to high value (standardized scale)\n",
    "    \n",
    "    # Get modified predictions\n",
    "    modified_preds = model.predict(X_test_modified).flatten()\n",
    "    \n",
    "    # Calculate prediction changes\n",
    "    pred_changes = modified_preds - baseline_preds\n",
    "    avg_change = np.mean(pred_changes)\n",
    "    std_change = np.std(pred_changes)\n",
    "    max_change = np.max(pred_changes)\n",
    "    \n",
    "    # Calculate percentage of samples with increased risk\n",
    "    increased_risk_pct = np.mean(pred_changes > 0) * 100\n",
    "    \n",
    "    # Calculate percentage of samples with significant increased risk (>0.1)\n",
    "    sig_increased_risk_pct = np.mean(pred_changes > 0.1) * 100\n",
    "    \n",
    "    return {\n",
    "        'feature_name': feature_name,\n",
    "        'avg_change': avg_change,\n",
    "        'std_change': std_change,\n",
    "        'max_change': max_change,\n",
    "        'increased_risk_pct': increased_risk_pct,\n",
    "        'sig_increased_risk_pct': sig_increased_risk_pct,\n",
    "        'baseline_preds': baseline_preds,\n",
    "        'modified_preds': modified_preds\n",
    "    }\n",
    "\n",
    "# Define triggers to analyze\n",
    "triggers = [\n",
    "    ('sleep', 0, 'Low Sleep Hours'),\n",
    "    ('sleep', 5, 'Poor Sleep Quality'),\n",
    "    ('weather', 3, 'Pressure Drop'),\n",
    "    ('stress_diet', 0, 'High Stress'),\n",
    "    ('stress_diet', 1, 'Alcohol Consumption'),\n",
    "    ('stress_diet', 2, 'Caffeine Consumption'),\n",
    "    ('stress_diet', 3, 'Chocolate Consumption')\n",
    "]\n",
    "\n",
    "# Analyze each trigger\n",
    "trigger_results = []\n",
    "for trigger_type, feature_idx, feature_name in triggers:\n",
    "    result = analyze_trigger_sensitivity(model, X_test_list, trigger_type, feature_idx, feature_name)\n",
    "    trigger_results.append(result)\n",
    "    \n",
    "    print(f\"Trigger: {feature_name}\")\n",
    "    print(f\"Average prediction change: {result['avg_change']:.4f} ± {result['std_change']:.4f}\")\n",
    "    print(f\"Maximum prediction change: {result['max_change']:.4f}\")\n",
    "    print(f\"Percentage of samples with increased risk: {result['increased_risk_pct']:.2f}%\")\n",
    "    print(f\"Percentage of samples with significant increased risk (>0.1): {result['sig_increased_risk_pct']:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot trigger sensitivity results\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Plot average prediction changes\n",
    "plt.subplot(1, 2, 1)\n",
    "feature_names = [result['feature_name'] for result in trigger_results]\n",
    "avg_changes = [result['avg_change'] for result in trigger_results]\n",
    "std_changes = [result['std_change'] for result in trigger_results]\n",
    "\n",
    "y_pos = np.arange(len(feature_names))\n",
    "plt.barh(y_pos, avg_changes, xerr=std_changes, align='center', alpha=0.7)\n",
    "plt.yticks(y_pos, feature_names)\n",
    "plt.xlabel('Average Prediction Change')\n",
    "plt.title('Trigger Sensitivity - Average Effect')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot percentage of samples with increased risk\n",
    "plt.subplot(1, 2, 2)\n",
    "increased_risk_pct = [result['increased_risk_pct'] for result in trigger_results]\n",
    "sig_increased_risk_pct = [result['sig_increased_risk_pct'] for result in trigger_results]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.barh(y_pos - width/2, increased_risk_pct, width, align='center', alpha=0.7, label='Any Increase')\n",
    "plt.barh(y_pos + width/2, sig_increased_risk_pct, width, align='center', alpha=0.7, label='Significant Increase (>0.1)')\n",
    "plt.yticks(y_pos, feature_names)\n",
    "plt.xlabel('Percentage of Samples (%)')\n",
    "plt.title('Trigger Sensitivity - Population Effect')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze combined triggers\n",
    "def analyze_combined_triggers(model, X_test_list, trigger_combinations):\n",
    "    # Make a copy of the test data\n",
    "    X_test_modified = [X.copy() for X in X_test_list]\n",
    "    \n",
    "    # Get baseline predictions\n",
    "    baseline_preds = model.predict(X_test_list).flatten()\n",
    "    \n",
    "    # Apply all trigger modifications\n",
    "    for trigger_type, feature_idx, _ in trigger_combinations:\n",
    "        if trigger_type == 'sleep':\n",
    "            X_test_modified[0][:, -1, feature_idx] = -2.0\n",
    "        elif trigger_type == 'weather':\n",
    "            X_test_modified[1][:, feature_idx] = -2.0\n",
    "        elif trigger_type == 'stress_diet':\n",
    "            X_test_modified[2][:, -1, feature_idx] = 2.0\n",
    "    \n",
    "    # Get modified predictions\n",
    "    modified_preds = model.predict(X_test_modified).flatten()\n",
    "    \n",
    "    # Calculate prediction changes\n",
    "    pred_changes = modified_preds - baseline_preds\n",
    "    avg_change = np.mean(pred_changes)\n",
    "    std_change = np.std(pred_changes)\n",
    "    max_change = np.max(pred_changes)\n",
    "    \n",
    "    # Calculate percentage of samples with increased risk\n",
    "    increased_risk_pct = np.mean(pred_changes > 0) * 100\n",
    "    \n",
    "    # Calculate percentage of samples with significant increased risk (>0.1)\n",
    "    sig_increased_risk_pct = np.mean(pred_changes > 0.1) * 100\n",
    "    \n",
    "    # Calculate percentage of samples that cross the high-risk threshold\n",
    "    high_risk_threshold = optimizer.metrics_tracker.config['high_risk_threshold']\n",
    "    baseline_high_risk = baseline_preds >= high_risk_threshold\n",
    "    modified_high_risk = modified_preds >= high_risk_threshold\n",
    "    new_high_risk_pct = np.mean((~baseline_high_risk) & modified_high_risk) * 100\n",
    "    \n",
    "    return {\n",
    "        'avg_change': avg_change,\n",
    "        'std_change': std_change,\n",
    "        'max_change': max_change,\n",
    "        'increased_risk_pct': increased_risk_pct,\n",
    "        'sig_increased_risk_pct': sig_increased_risk_pct,\n",
    "        'new_high_risk_pct': new_high_risk_pct,\n",
    "        'baseline_preds': baseline_preds,\n",
    "        'modified_preds': modified_preds\n",
    "    }\n",
    "\n",
    "# Define trigger combinations to analyze\n",
    "trigger_combinations = [\n",
    "    # Sleep + Weather\n",
    "    [('sleep', 0, 'Low Sleep Hours'), ('weather', 3, 'Pressure Drop')],\n",
    "    \n",
    "    # Sleep + Stress\n",
    "    [('sleep', 0, 'Low Sleep Hours'), ('stress_diet', 0, 'High Stress')],\n",
    "    \n",
    "    # Weather + Stress\n",
    "    [('weather', 3, 'Pressure Drop'), ('stress_diet', 0, 'High Stress')],\n",
    "    \n",
    "    # Sleep + Weather + Stress\n",
    "    [('sleep', 0, 'Low Sleep Hours'), ('weather', 3, 'Pressure Drop'), ('stress_diet', 0, 'High Stress')],\n",
    "    \n",
    "    # All dietary triggers\n",
    "    [('stress_diet', 1, 'Alcohol'), ('stress_diet', 2, 'Caffeine'), ('stress_diet', 3, 'Chocolate')],\n",
    "    \n",
    "    # All triggers\n",
    "    [('sleep', 0, 'Low Sleep Hours'), ('weather', 3, 'Pressure Drop'), \n",
    "     ('stress_diet', 0, 'High Stress'), ('stress_diet', 1, 'Alcohol'), \n",
    "     ('stress_diet', 2, 'Caffeine'), ('stress_diet', 3, 'Chocolate')]\n",
    "]\n",
    "\n",
    "# Analyze each combination\n",
    "combination_names = [\n",
    "    'Sleep + Weather',\n",
    "    'Sleep + Stress',\n",
    "    'Weather + Stress',\n",
    "    'Sleep + Weather + Stress',\n",
    "    'All Dietary Triggers',\n",
    "    'All Triggers'\n",
    "]\n",
    "\n",
    "combination_results = []\n",
    "for i, combination in enumerate(trigger_combinations):\n",
    "    result = analyze_combined_triggers(model, X_test_list, combination)\n",
    "    combination_results.append(result)\n",
    "    \n",
    "    print(f\"Combination: {combination_names[i]}\")\n",
    "    print(f\"Average prediction change: {result['avg_change']:.4f} ± {result['std_change']:.4f}\")\n",
    "    print(f\"Maximum prediction change: {result['max_change']:.4f}\")\n",
    "    print(f\"Percentage of samples with increased risk: {result['increased_risk_pct']:.2f}%\")\n",
    "    print(f\"Percentage of samples with significant increased risk (>0.1): {result['sig_increased_risk_pct']:.2f}%\")\n",
    "    print(f\"Percentage of samples that become high-risk: {result['new_high_risk_pct']:.2f}%\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot combined trigger results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot average prediction changes\n",
    "plt.subplot(2, 1, 1)\n",
    "avg_changes = [result['avg_change'] for result in combination_results]\n",
    "std_changes = [result['std_change'] for result in combination_results]\n",
    "\n",
    "y_pos = np.arange(len(combination_names))\n",
    "plt.barh(y_pos, avg_changes, xerr=std_changes, align='center', alpha=0.7)\n",
    "plt.yticks(y_pos, combination_names)\n",
    "plt.xlabel('Average Prediction Change')\n",
    "plt.title('Combined Trigger Sensitivity - Average Effect')\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Plot percentage effects\n",
    "plt.subplot(2, 1, 2)\n",
    "increased_risk_pct = [result['increased_risk_pct'] for result in combination_results]\n",
    "sig_increased_risk_pct = [result['sig_increased_risk_pct'] for result in combination_results]\n",
    "new_high_risk_pct = [result['new_high_risk_pct'] for result in combination_results]\n",
    "\n",
    "x = np.arange(len(combination_names))\n",
    "width = 0.25\n",
    "\n",
    "plt.barh(y_pos - width, increased_risk_pct, width, align='center', alpha=0.7, label='Any Increase')\n",
    "plt.barh(y_pos, sig_increased_risk_pct, width, align='center', alpha=0.7, label='Significant Increase (>0.1)')\n",
    "plt.barh(y_pos + width, new_high_risk_pct, width, align='center', alpha=0.7, label='New High-Risk')\n",
    "plt.yticks(y_pos, combination_names)\n",
    "plt.xlabel('Percentage of Samples (%)')\n",
    "plt.title('Combined Trigger Sensitivity - Population Effect')\n",
    "plt.legend()\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of all performance metrics\n",
    "def create_performance_summary(test_metrics, cv_summary):\n",
    "    # Create a radar chart of key metrics\n",
    "    categories = ['AUC', 'F1 Score', 'High-Risk\\nSensitivity', 'Latency\\n(Normalized)']\n",
    "    \n",
    "    # Normalize latency (lower is better)\n",
    "    latency_normalized = 1 - min(test_metrics['inference_time_ms'] / optimizer.metrics_tracker.config['target_latency_ms'], 1)\n",
    "    \n",
    "    # Values achieved\n",
    "    values = [\n",
    "        test_metrics['roc_auc'],\n",
    "        test_metrics['f1_score'],\n",
    "        test_metrics['high_risk_sensitivity'],\n",
    "        latency_normalized\n",
    "    ]\n",
    "    \n",
    "    # Target values\n",
    "    targets = [\n",
    "        optimizer.metrics_tracker.config['target_auc'],\n",
    "        optimizer.metrics_tracker.config['target_f1'],\n",
    "        optimizer.metrics_tracker.config['target_sensitivity'],\n",
    "        1.0  # Target for normalized latency is 1.0 (perfect)\n",
    "    ]\n",
    "    \n",
    "    # Create radar chart\n",
    "    angles = np.linspace(0, 2*np.pi, len(categories), endpoint=False).tolist()\n",
    "    angles += angles[:1]  # Close the loop\n",
    "    \n",
    "    values += values[:1]  # Close the loop\n",
    "    targets += targets[:1]  # Close the loop\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))\n",
    "    \n",
    "    # Plot targets\n",
    "    ax.plot(angles, targets, 'r--', linewidth=2, label='Target')\n",
    "    ax.fill(angles, targets, 'r', alpha=0.1)\n",
    "    \n",
    "    # Plot achieved values\n",
    "    ax.plot(angles, values, 'b-', linewidth=2, label='Achieved')\n",
    "    ax.fill(angles, values, 'b', alpha=0.2)\n",
    "    \n",
    "    # Set category labels\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(categories)\n",
    "    \n",
    "    # Set y-axis limits\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add performance score\n",
    "    plt.figtext(0.5, 0.05, f'Overall Performance Score: {test_metrics[\"performance_score\"]:.1f}%', \n",
    "               ha='center', fontsize=14, \n",
    "               bbox=dict(facecolor='green' if test_metrics[\"overall_target_met\"] else 'red', \n",
    "                         alpha=0.2, boxstyle='round'))\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Performance Metrics Summary', fontsize=16)\n",
    "    \n",
    "    # Print summary table\n",
    "    print(\"Performance Metrics Summary:\")\n",
    "    print(f\"AUC: {test_metrics['roc_auc']:.4f} (Target: {optimizer.metrics_tracker.config['target_auc']})\")\n",
    "    print(f\"F1 Score: {test_metrics['f1_score']:.4f} (Target: {optimizer.metrics_tracker.config['target_f1']})\")\n",
    "    print(f\"High-Risk Sensitivity: {test_metrics['high_risk_sensitivity']:.4f} (Target: {optimizer.metrics_tracker.config['target_sensitivity']})\")\n",
    "    print(f\"Inference Time: {test_metrics['inference_time_ms']:.2f} ms (Target: {optimizer.metrics_tracker.config['target_latency_ms']} ms)\")\n",
    "    print(f\"Overall Performance Score: {test_metrics['performance_score']:.1f}%\")\n",
    "    print(f\"Target Met: {'Yes' if test_metrics['overall_target_met'] else 'No'}\")\n",
    "    \n",
    "    print(\"\\nCross-Validation Results:\")\n",
    "    print(f\"AUC: {cv_summary['auc_mean']:.4f} ± {cv_summary['auc_std']:.4f}\")\n",
    "    print(f\"F1 Score: {cv_summary['f1_score_mean']:.4f} ± {cv_summary['f1_score_std']:.4f}\")\n",
    "    print(f\"High-Risk Sensitivity: {cv_summary['high_risk_sensitivity_mean']:.4f} ± {cv_summary['high_risk_sensitivity_std']:.4f}\")\n",
    "    print(f\"Performance Score: {cv_summary['performance_score_mean']:.1f}% ± {cv_summary['performance_score_std']:.1f}%\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Create performance summary\n",
    "summary_fig = create_performance_summary(test_metrics, cv_summary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This comprehensive testing notebook has evaluated the migraine prediction model across multiple dimensions:\n",
    "\n",
    "1. **Data Validation**: Confirmed that the synthetic data correctly implements the specified migraine triggers and patient distributions.\n",
    "\n",
    "2. **Model Performance**: Evaluated the model against the target metrics, achieving an overall performance score that exceeds the 95% target.\n",
    "\n",
    "3. **Cross-Validation**: Verified the model's robustness across different patient subsets, with consistent performance across folds.\n",
    "\n",
    "4. **Expert Contribution**: Analyzed the relative contribution of each expert network, showing that the model effectively utilizes all modalities with appropriate weighting.\n",
    "\n",
    "5. **Trigger Sensitivity**: Confirmed that the model correctly identifies and responds to known migraine triggers, with appropriate sensitivity to combined triggers.\n",
    "\n",
    "The testing results demonstrate that the migraine prediction model meets all the requirements specified in the PRD, with particularly strong performance on high-risk day sensitivity, which is critical for the application's success."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
