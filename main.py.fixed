import os
import sys
import time
import json
import logging
import argparse
from pathlib import Path
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Optional, Union
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from scipy import stats
import seaborn as sns
import matplotlib
matplotlib.use('TkAgg')  # Try 'MacOSX' or 'Qt5Agg' if this doesn't work
from concurrent.futures import ProcessPoolExecutor
from functools import partial

# Import local modules from root directory
from meta.meta_optimizer import MetaOptimizer
from meta.meta_learner import MetaLearner
from models.model_factory import ModelFactory
from explainability.explainer_factory import ExplainerFactory
from optimizers.optimizer_factory import (
    OptimizerFactory,
    DifferentialEvolutionOptimizer, 
    EvolutionStrategyOptimizer,
    AntColonyOptimizer,
    GreyWolfOptimizer
)
from visualization.drift_analysis import DriftAnalyzer
from visualization.optimizer_analysis import OptimizerAnalyzer
from evaluation.framework_evaluator import FrameworkEvaluator
from drift_detection.drift_detector import DriftDetector
from visualization.live_visualization import LiveOptimizationMonitor
from benchmarking.test_functions import TEST_FUNCTIONS, create_test_suite
from utils.plot_utils import save_plot

# Import migraine data handling modules
try:
    from migraine_prediction_project.src.migraine_model.new_data_migraine_predictor import MigrainePredictorV2
    from migraine_prediction_project.src.migraine_model.data_handler import DataHandler
    MIGRAINE_MODULES_AVAILABLE = True
except ImportError:
    MIGRAINE_MODULES_AVAILABLE = False
    logging.warning("Migraine prediction modules not available. To use migraine data features, please install the migraine prediction package.")

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

"""
Comprehensive benchmark and comparison of optimization algorithms
including meta-optimization for novel algorithm creation.
"""

# Create benchmark functions dictionary
benchmark_functions = TEST_FUNCTIONS

# Setup logging and directories
def setup_environment():
    """Configure logging and create necessary directories"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('optimization.log'),
            logging.StreamHandler()
        ]
    )
    
    # Create directories
    directories = ['results', 'results/plots', 'results/data', 'results/explainability']
    for directory in directories:
        Path(directory).mkdir(exist_ok=True, parents=True)

# Create optimizers for benchmarking
def create_optimizers(dim: int, bounds: List[Tuple[float, float]], verbose: bool = True) -> Dict[str, Any]:
    """Create all optimizer instances for benchmarking"""
    factory = OptimizerFactory()
    
    return {
        'ACO': factory.create_optimizer('ant_colony', dim=dim, bounds=bounds, name="ACO", verbose=verbose),
        'GWO': factory.create_optimizer('grey_wolf', dim=dim, bounds=bounds, name="GWO", verbose=verbose),
        'DE': factory.create_optimizer('differential_evolution', dim=dim, bounds=bounds, name="DE", verbose=verbose),
        'ES': factory.create_optimizer('evolution_strategy', dim=dim, bounds=bounds, name="ES", verbose=verbose),
        'DE (Adaptive)': factory.create_optimizer('differential_evolution', dim=dim, bounds=bounds, adaptive=True, name="DE (Adaptive)", verbose=verbose),
        'ES (Adaptive)': factory.create_optimizer('evolution_strategy', dim=dim, bounds=bounds, adaptive=True, name="ES (Adaptive)", verbose=verbose),
        'Meta-Optimizer': MetaOptimizer(
            dim=dim, 
            bounds=bounds, 
            optimizers={
                'DE': factory.create_optimizer('differential_evolution', dim=dim, bounds=bounds),
                'ES': factory.create_optimizer('evolution_strategy', dim=dim, bounds=bounds)
            },
            verbose=verbose
        )
    }

# Run optimization
def run_optimization(
    args,
    dim: int = 30, 
    max_evals: int = 10000, 
    n_runs: int = 5, 
    live_viz: bool = False, 
    save_plots: bool = True
) -> Tuple[Dict[str, Any], pd.DataFrame]:
    """
    Run optimization process with multiple optimizers and test functions.
    
    Args:
        args: Command-line arguments (can be None if calling directly)
        dim: Dimensionality of the optimization problem
        max_evals: Maximum number of function evaluations
        n_runs: Number of independent runs per optimizer
        live_viz: Whether to enable live visualization
        save_plots: Whether to save plots
        
    Returns:
        Tuple of (results dictionary, results dataframe)
    """
    logging.info("Starting optimization process")
    
    # Process args if provided
    if hasattr(args, 'visualize'):
        live_viz = args.visualize
    
    # Create results directories
    results_dir = Path('results')
    data_dir = results_dir / 'data'
    plots_dir = results_dir / 'performance'
    
    for directory in [results_dir, data_dir, plots_dir]:
        directory.mkdir(exist_ok=True, parents=True)
    
    # Define test functions
    test_suite = create_test_suite()
    selected_functions = {
        'unimodal': ['sphere', 'rosenbrock'],
        'multimodal': ['rastrigin', 'ackley'],
    }
    
    # Prepare benchmark functions
    benchmark_functions = {}
    for category, functions in selected_functions.items():
        for func_name in functions:
            func_creator = TEST_FUNCTIONS[func_name]
            bounds = [(-5, 5)] * dim  # Default bounds
            benchmark_functions[func_name] = func_creator(dim, bounds)
    
    # Create optimizers
    bounds = [(-5, 5)] * dim
    optimizers = create_optimizers(dim, bounds)
    
    # Create meta-optimizer
    meta_opt = MetaOptimizer(
        dim=dim,
        bounds=bounds,
        optimizers=optimizers,
        history_file=str(data_dir / 'meta_history.json'),
        selection_file=str(data_dir / 'meta_selection.json')
    )
    
    # Enable live visualization if requested
    if live_viz:
        meta_opt.enable_live_visualization(str(plots_dir) if save_plots else None)
    
    # Add meta-optimizer to the comparison
    all_optimizers = optimizers.copy()
    all_optimizers['Meta-Optimizer'] = meta_opt
    
    # Create analyzer and run comparison
    analyzer = OptimizerAnalyzer(all_optimizers)
    results = {}
    all_results_data = []
    
    # Run optimization for each test function
    for func_name, func in benchmark_functions.items():
        logging.info(f"Optimizing {func_name} function")
        
        function_results = analyzer.run_comparison(
            {func_name: func},
            n_runs=n_runs,
            record_convergence=True,
            max_evals=max_evals
        )
        
        # Store results
        results[func_name] = function_results[func_name]
        
        # Collect data for summary dataframe
        for opt_name, opt_results in function_results[func_name].items():
            for run, result in enumerate(opt_results):
                all_results_data.append({
                    'function': func_name,
                    'dimension': dim,
                    'optimizer': opt_name,
                    'run': run,
                    'best_score': result.best_score,
                    'execution_time': result.execution_time,
                    'convergence_length': len(result.convergence_curve)
                })
        
        # Generate plots for this function
        if save_plots:
            # Plot convergence
            fig = analyzer.plot_convergence_comparison()
            if fig:  # Only save if a figure was returned
                save_plot(fig, f"{func_name}_convergence.png", plot_type='performance')
                plt.close(fig)
            
            # Plot parameter adaptation and diversity for each optimizer
            for opt_name in all_optimizers:
                try:
                    fig = analyzer.plot_parameter_adaptation(opt_name, func_name)
                    if fig:  # Only save if a figure was returned
                        save_plot(fig, f"{func_name}_{opt_name}_parameters.png", plot_type='performance')
                        plt.close(fig)
                except Exception as e:
                    logging.warning(f"Could not plot parameter adaptation for {opt_name}: {str(e)}")
                
                try:
                    fig = analyzer.plot_diversity_analysis(opt_name, func_name)
                    if fig:  # Only save if a figure was returned
                        save_plot(fig, f"{func_name}_{opt_name}_diversity.png", plot_type='performance')
                        plt.close(fig)
                except Exception as e:
                    logging.warning(f"Could not plot diversity analysis for {opt_name}: {str(e)}")
    
    # Create overall performance heatmap
    if save_plots:
        try:
            analyzer.plot_performance_heatmap()
        except Exception as e:
            logging.warning(f"Could not plot performance heatmap: {str(e)}")
        
        # Create boxplot of performance across functions
        try:
            plt.figure(figsize=(14, 10))
            sns.boxplot(data=pd.DataFrame(all_results_data), x='optimizer', y='best_score', hue='function')
            plt.yscale('log')
            plt.title('Performance Comparison Across Functions')
            plt.xticks(rotation=45)
            plt.tight_layout()
            fig = plt.gcf()
            save_plot(fig, "performance_boxplot.png", plot_type='performance')
            plt.close(fig)
        except Exception as e:
            logging.warning(f"Could not plot performance boxplot: {str(e)}")
    
    # Create and save summary dataframe
    results_df = pd.DataFrame(all_results_data)
    results_df.to_csv(data_dir / 'optimization_results.csv', index=False)
    
    # Create summary statistics
    summary_stats = results_df.groupby(['function', 'optimizer']).agg({
        'best_score': ['mean', 'std', 'min'],
        'execution_time': ['mean', 'std']
    }).reset_index()
    
    summary_stats.to_csv(data_dir / 'optimization_summary.csv')
    
    # Prepare final results
    final_results = {
        'best_score': results_df['best_score'].min(),
        'best_optimizer': results_df.loc[results_df['best_score'].idxmin(), 'optimizer'],
        'best_function': results_df.loc[results_df['best_score'].idxmin(), 'function'],
        'summary_stats': summary_stats,
        'optimizers': list(all_optimizers.keys())
    }
    
    return final_results, results_df

# Run benchmark comparison
def run_benchmark_comparison(n_runs: int = 30, max_evals: int = 10000, live_viz: bool = False, save_plots: bool = True):
    """Run comprehensive benchmark comparison of all optimizers"""
    logging.info("Starting benchmark comparison")
    
    # Get test functions
    test_suite = create_test_suite()
    selected_functions = {
        'unimodal': ['sphere', 'rosenbrock'],
        'multimodal': ['rastrigin', 'ackley', 'griewank'],
        'hybrid': ['levy']
    }
    
    # Prepare benchmark functions
    benchmark_functions = {}
    for category, functions in selected_functions.items():
        for func_name in functions:
            for dim in [10, 30]:
                key = f"{func_name}_{dim}D"
                func_creator = TEST_FUNCTIONS[func_name]
                bounds = [(-5, 5)] * dim  # Default bounds
                benchmark_functions[key] = func_creator(dim, bounds)
    
    # Run comparison for each function
    results = {}
    all_results_data = []
    
    for func_name, func in benchmark_functions.items():
        logging.info(f"Benchmarking function: {func_name}")
        
        # Create optimizers for this dimension
        dim = func.dim
        bounds = func.bounds
        optimizers = create_optimizers(dim, bounds)
        
        # Create meta-optimizer
        meta_opt = MetaOptimizer(
            dim=dim,
            bounds=bounds,
            optimizers=optimizers,
            history_file=f'results/data/meta_history_{func_name}.json'
        )
        
        # Enable live visualization if requested
        if live_viz:
            save_path = 'results/plots' if save_plots else None
            meta_opt.enable_live_visualization(save_path)
            
        # Add meta-optimizer to the comparison
        all_optimizers = optimizers.copy()
        all_optimizers['Meta-Optimizer'] = meta_opt
        
        # Create analyzer and run comparison
        analyzer = OptimizerAnalyzer(all_optimizers)
        function_results = analyzer.run_comparison(
            {func_name: func},
            n_runs=n_runs,
            record_convergence=True,
            max_evals=max_evals
        )
        
        # Store results
        results[func_name] = function_results[func_name]
        
        # Collect data for summary dataframe
        for opt_name, opt_results in function_results[func_name].items():
            for run, result in enumerate(opt_results):
                all_results_data.append({
                    'function': func_name,
                    'dimension': dim,
                    'optimizer': opt_name,
                    'run': run,
                    'best_score': result.best_score,
                    'execution_time': result.execution_time,
                    'convergence_length': len(result.convergence_curve)
                })
        
        # Generate plots for this function
        if save_plots:
            analyzer.plot_convergence_comparison()
            for opt_name in all_optimizers:
                try:
                    analyzer.plot_parameter_adaptation(opt_name, func_name)
                except:
                    logging.warning(f"Could not plot parameter adaptation for {opt_name}")
                
                try:
                    analyzer.plot_diversity_analysis(opt_name, func_name)
                except:
                    logging.warning(f"Could not plot diversity analysis for {opt_name}")
    
    # Create overall performance heatmap
    if save_plots:
        analyzer.plot_performance_heatmap()
    
    # Create and save summary dataframe
    results_df = pd.DataFrame(all_results_data)
    results_df.to_csv('results/data/benchmark_results.csv', index=False)
    
    # Create summary statistics
    summary_stats = results_df.groupby(['function', 'optimizer']).agg({
        'best_score': ['mean', 'std', 'min'],
        'execution_time': ['mean', 'std']
    }).reset_index()
    
    summary_stats.to_csv('results/data/benchmark_summary.csv')
    
    # Create additional visualizations
    if save_plots:
        plt.figure(figsize=(14, 10))
        sns.boxplot(data=results_df, x='optimizer', y='best_score', hue='function')
        plt.yscale('log')
        plt.title('Performance Comparison Across Functions')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig('results/plots/performance_boxplot.png')
        plt.close()
    
    return results, results_df

# Run meta-learner with drift detection
def run_meta_learner_with_drift(n_samples=1000, n_features=10, drift_points=None, 
                              window_size=50, drift_threshold=0.5, significance_level=0.05, 
                              min_drift_interval=30, ema_alpha=0.3, visualize=False):
    """Run meta-learner with drift detection."""
    logging.info("Running meta-learner with drift detection")
    
    # Generate synthetic data
    X_full, y_full, drift_points = generate_synthetic_data_with_drift(
        n_samples=n_samples,
        n_features=n_features,
        drift_points=[n_samples//4, n_samples//2, 3*n_samples//4]  # Fixed drift points
    )
    logging.info(f"Generated synthetic data with drift points at: {drift_points}")
    
    # Initialize meta-learner
    meta_learner = MetaLearner(
        method='bayesian',
        surrogate_model=None,
        selection_strategy='bandit',
        exploration_factor=0.2,
        history_weight=0.7
    )
    
    # Import the correct DriftDetector class from drift_detection
    from drift_detection.drift_detector import DriftDetector
    
    # Initialize drift detector with enhanced parameters
    detector = DriftDetector(
        window_size=window_size,
        drift_threshold=drift_threshold,
        significance_level=significance_level,
        min_drift_interval=min_drift_interval,
        ema_alpha=ema_alpha,
        max_history_size=100  # Add max_history_size parameter
    )
    
    # Set the drift detector for the meta-learner
    meta_learner.drift_detector = detector
    
    # Add some algorithms to the meta-learner
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import LinearRegression
    
    # Create simple algorithm wrappers
    class Algorithm:
        def __init__(self, name, model):
            self.name = name
            self.model = model
            self.best_config = None
            self.best_score = float('-inf')
            self.is_fitted = False
            
        def fit(self, X, y):
            self.model.fit(X, y)
            self.is_fitted = True
            
        def predict(self, X):
            if not self.is_fitted:
                logging.warning(f"{self.name} model is not fitted yet. Fitting with dummy data before prediction.")
                # Create dummy data to fit the model
                import numpy as np
                dummy_X = np.random.random((10, X.shape[1]))
                dummy_y = np.random.random(10)
                self.fit(dummy_X, dummy_y)
                logging.warning(f"{self.name} model fitted with dummy data. Predictions may not be meaningful.")
            return self.model.predict(X)
            
        def suggest(self, num_points=1):
            """Suggest next points to evaluate (dummy implementation)"""
            import numpy as np
            # Check if we should return a single point or multiple
            if num_points == 1:
                return np.random.uniform(-1, 1, (1, 10))  # Return as 2D array with 1 row
            # Otherwise return multiple points
            return np.random.uniform(-1, 1, (num_points, 10))
            
        def evaluate(self, X):
            """Evaluate points for meta-learning (dummy implementation)"""
            # Return dummy scores for the points
            import numpy as np
            # Make sure X is 2D when computing shape
            if isinstance(X, np.ndarray):
                if X.ndim == 1:
                    X = X.reshape(1, -1)
                # Ensure we return 1D array for multiple points or scalar for single point
                if X.shape[0] > 1:
                    return np.random.uniform(0, 1, X.shape[0]).astype(float)
                else:
                    return float(np.random.uniform(0, 1))
            else:
                # Handle scalar case
                return float(np.random.uniform(0, 1))
            
        def update(self, X, scores=None):
            """Update the optimizer with new data (dummy implementation)"""
            import numpy as np
            
            # Always treat X and scores as arrays for consistency
            try:
                # Handle scalar X by converting to a 1x1 array
                if np.isscalar(X):
                    X = np.array([[float(X)]])
                elif isinstance(X, np.ndarray):
                    if X.ndim == 0:  # Handle numpy scalars
                        X = np.array([[float(X)]])
                    elif X.ndim == 1:
                        X = X.reshape(1, -1)
                        
                # Just record best score if scores are provided
                if scores is not None:
                    # Ensure scores is a float array
                    if np.isscalar(scores):
                        scores = np.array([float(scores)])
                    else:
                        scores = np.asarray(scores, dtype=float)
                        if scores.ndim == 0:  # scalar
                            scores = np.array([float(scores)])
                        
                    if scores.size > 0:
                        # Ensure best_score is initialized
                        if self.best_score == float('-inf'):
                            self.best_score = float(scores.max())
                            best_idx = scores.argmax() if scores.size > 1 else 0
                            self.best_config = X[best_idx].copy() if best_idx < len(X) else X[0].copy()
                        elif scores.max() > self.best_score:
                            self.best_score = float(scores.max())
                            best_idx = scores.argmax() if scores.size > 1 else 0
                            self.best_config = X[best_idx].copy() if best_idx < len(X) else X[0].copy()
                # Otherwise fit the model with new data
                else:
                    self.fit(X, scores)
            except Exception as e:
                logging.error(f"Error in update: {str(e)}")
                # Fallback to prevent crashes
                if self.best_score == float('-inf'):
                    self.best_score = 0.0
                    if isinstance(X, np.ndarray) and X.size > 0:
                        self.best_config = X[0].copy() if X.ndim > 1 else X.copy()
            
        def get_best(self):
            """Get best configuration found so far"""
            return self.best_config, self.best_score
        
    # Add algorithms to meta-learner
    meta_learner.algorithms = [
        Algorithm("RF", RandomForestRegressor(n_estimators=100, random_state=42)),
        Algorithm("GB", GradientBoostingRegressor(n_estimators=100, random_state=42)),
        Algorithm("LR", LinearRegression())
    ]
    
    # Set algorithms
    from optimizers.optimizer_factory import create_optimizers
    optimizers = create_optimizers(dim=n_features, bounds=[(-5, 5)] * n_features)
    meta_learner.set_algorithms(list(optimizers.values()))
    
    # Initialize with first 200 samples
    X_init, y_init = X_full[:200], y_full[:200]
    meta_learner.fit(X_init, y_init)
    
    # Get best configuration
    best_config = meta_learner.best_config
    
    # Process data in chunks and check for drift
    chunk_size = 50
    detected_drifts = []
    all_predictions = []
    all_true_values = []
    all_errors = []
    all_severities = []
    
    for i in range(4, n_samples // chunk_size):
        start_idx = i * chunk_size
        end_idx = min((i + 1) * chunk_size, n_samples)
        X_chunk, y_chunk = X_full[start_idx:end_idx], y_full[start_idx:end_idx]
        
        # Check for drift
        try:
            drift_detected = meta_learner.update(X_chunk, y_chunk)
            if drift_detected:
                detected_drifts.append(start_idx)
                logging.info(f"Drift detected at sample {start_idx}")
        except Exception as e:
            logging.error(f"Error in update: {str(e)}")
        
        # Log statistics every 100 samples
        if i % 2 == 0:
            try:
                # Make predictions
                y_pred = meta_learner.predict(X_chunk)
                all_predictions.extend(y_pred)
                all_true_values.extend(y_chunk)
                
                # Calculate statistics
                from scipy import stats
                errors = y_chunk - y_pred
                all_errors.extend(errors)
                
                mean_shift = np.abs(np.mean(errors))
                
                # KS test with uniform distribution as reference
                ks_stat, p_value = stats.kstest(errors, 'uniform')
                
                # Calculate severity
                severity = mean_shift * ks_stat
                all_severities.append((start_idx, severity))
                
                logging.info(f"Sample {start_idx}: Mean shift={mean_shift:.4f}, KS stat={ks_stat:.4f}, p-value={p_value:.4f}, Severity={severity:.4f}")
            except Exception as e:
                logging.error(f"Prediction error: {str(e)}")
    
    # Manually check for drift at known drift points
    manual_drift_checks = []
    for drift_point in drift_points:
        logging.info(f"Manually checking for drift at point {drift_point}...")
        
        # Get data before and after drift point
        before_start = max(0, drift_point - window_size * 2)
        after_end = min(n_samples, drift_point + window_size * 2)
        
        X_before = X_full[before_start:drift_point]
        y_before = y_full[before_start:drift_point]
        X_after = X_full[drift_point:after_end]
        y_after = y_full[drift_point:after_end]
        
        # Check for drift
        drift_detected, mean_shift, ks_statistic, p_value = check_drift_at_point(
            meta_learner, X_before, y_before, X_after, y_after, 
            window_size=window_size, drift_threshold=drift_threshold, 
            significance_level=significance_level
        )
        
        manual_drift_checks.append({
            'point': drift_point,
            'detected': drift_detected,
            'mean_shift': mean_shift,
            'ks_stat': ks_statistic,
            'p_value': p_value,
            'severity': mean_shift * ks_statistic
        })
        
        if drift_detected:
            logging.info(f"Drift check at {drift_point}: Detected")
        else:
            logging.info(f"Drift check at {drift_point}: Not detected")
            
        logging.info(f"Stats: Mean shift={mean_shift:.4f}, KS stat={ks_statistic:.4f}, p-value={p_value:.6f}")
    
    # Get feature importance
    try:
        feature_importance = meta_learner.get_feature_importance()
        feature_names = [f"feature_{i}" for i in range(n_features)]
        feature_importance_dict = dict(zip(feature_names, feature_importance))
    except Exception as e:
        logging.warning(f"Could not retrieve feature importance")
        feature_importance_dict = {}
    
    # Visualize results if requested
    if visualize:
        try:
            import matplotlib.pyplot as plt
            import os
            
            # Create results directory if it doesn't exist
            os.makedirs('results', exist_ok=True)
            
            # Plot 1: True values vs Predictions
            plt.figure(figsize=(12, 6))
            plt.plot(all_true_values, label='True Values')
            plt.plot(all_predictions, label='Predictions')
            for drift_point in drift_points:
                plt.axvline(x=drift_point - 200, color='r', linestyle='--', alpha=0.5)
            for detected in detected_drifts:
                plt.axvline(x=detected - 200, color='g', linestyle=':', alpha=0.5)
            plt.legend()
            plt.title('True Values vs Predictions')
            plt.xlabel('Sample Index')
            plt.ylabel('Value')
            save_plot(plt.gcf(), 'drift_true_vs_pred.png', plot_type='drift')
            plt.close()
            
            # Plot 2: Prediction Errors
            plt.figure(figsize=(12, 6))
            plt.plot(all_errors)
            for drift_point in drift_points:
                plt.axvline(x=drift_point - 200, color='r', linestyle='--', alpha=0.5, label='Known Drift' if drift_point == drift_points[0] else None)
            for detected in detected_drifts:
                plt.axvline(x=detected - 200, color='g', linestyle=':', alpha=0.5, label='Detected Drift' if detected == detected_drifts[0] else None)
            plt.legend()
            plt.title('Prediction Errors')
            plt.xlabel('Sample Index')
            plt.ylabel('Error')
            save_plot(plt.gcf(), 'drift_errors.png', plot_type='drift')
            plt.close()
            
            # Plot 3: Drift Severity
            plt.figure(figsize=(12, 6))
            indices = [x[0] for x in all_severities]
            severities = [x[1] for x in all_severities]
            plt.plot(indices, severities, marker='o')
            plt.axhline(y=drift_threshold, color='r', linestyle='--', label='Threshold')
            for drift_point in drift_points:
                plt.axvline(x=drift_point, color='r', linestyle='--', alpha=0.5, label='Known Drift' if drift_point == drift_points[0] else None)
            for detected in detected_drifts:
                plt.axvline(x=detected, color='g', linestyle=':', alpha=0.5, label='Detected Drift' if detected == detected_drifts[0] else None)
            plt.legend()
            plt.title('Drift Severity Over Time')
            plt.xlabel('Sample Index')
            plt.ylabel('Severity')
            save_plot(plt.gcf(), 'drift_severity.png', plot_type='drift')
            plt.close()
            
            # Plot 4: Feature Importance
            if feature_importance_dict:
                plt.figure(figsize=(12, 6))
                features = list(feature_importance_dict.keys())
                importances = list(feature_importance_dict.values())
                plt.bar(features, importances)
                plt.title('Feature Importance')
                plt.xlabel('Feature')
                plt.ylabel('Importance')
                plt.xticks(rotation=45)
                plt.tight_layout()
                save_plot(plt.gcf(), 'feature_importance.png', plot_type='explainability')
                plt.close()
            
            logging.info("Visualizations saved to results directory")
        except Exception as e:
            logging.error(f"Error generating visualizations: {str(e)}")
    
    # Return results
    results = {
        "detected_drifts": detected_drifts,
        "known_drift_points": drift_points,
        "feature_importance": feature_importance_dict,
        "best_config": best_config,
        "manual_drift_checks": manual_drift_checks
    }
    
    return results

def generate_synthetic_data_with_drift(n_samples=1000, n_features=10, drift_points=None, noise_level=0.1):
    """Generate synthetic data with concept drift at specified points."""
    # Generate random drift points if not specified
    if drift_points is None:
        drift_points = sorted(np.random.choice(
            range(100, n_samples - 100), 
            size=3, 
            replace=False
        ))
    
    logging.info(f"Generated synthetic data with drift points at: {drift_points}")
    
    # Generate features
    X = np.random.randn(n_samples, n_features)
    
    # Generate target with concept drift
    y = np.zeros(n_samples)
    
    # Initial coefficients
    coef = np.random.randn(n_features)
    
    # Generate data with different coefficients for each segment
    start_idx = 0
    for drift_point in drift_points:
        # Apply current coefficients to this segment
        y[start_idx:drift_point] = np.dot(X[start_idx:drift_point], coef) + noise_level * np.random.randn(drift_point - start_idx)
        
        # Create more pronounced drift by significantly changing coefficients
        # Increase the magnitude of change at drift points
        coef = coef + 1.5 * np.random.randn(n_features)  # Increased from 0.5 to 1.5
        
        # Add an abrupt shift at the drift point
        shift_magnitude = 0.5 + 0.5 * np.random.random()  # Random shift between 0.5 and 1.0
        if drift_point < n_samples:
            # Apply an abrupt shift to a small window after the drift point
            post_drift_window = min(20, n_samples - drift_point)
            y[drift_point:drift_point+post_drift_window] += shift_magnitude
        
        start_idx = drift_point
    
    # Last segment
    y[start_idx:] = np.dot(X[start_idx:], coef) + noise_level * np.random.randn(n_samples - start_idx)
    
    return X, y, drift_points

def check_drift_at_point(meta_learner, X_before, y_before, X_after, y_after, window_size=10, drift_threshold=0.01, significance_level=0.9):
    """Check for drift at a specific point using data before and after the point."""
    # Create a separate drift detector with sensitive parameters
    drift_detector = DriftDetector(
        window_size=window_size,
        drift_threshold=drift_threshold,
        significance_level=significance_level,
        min_drift_interval=1,  # Allow immediate detection
        ema_alpha=0.9  # High alpha for quick response
    )
    
    # Make predictions
    try:
        y_pred_before = meta_learner.predict(X_before)
        y_pred_after = meta_learner.predict(X_after)
    except Exception as e:
        logging.error(f"Prediction error in drift check: {str(e)}")
        # Skip the meta_learner fallback and go straight to a simple model
        # This avoids any potential issues with invalid parameters
        from sklearn.ensemble import RandomForestRegressor
        simple_model = RandomForestRegressor(
            n_estimators=10,
            max_depth=5,
            min_samples_split=2,
            min_samples_leaf=1,
            max_features='sqrt',
            random_state=42
        )
        try:
            simple_model.fit(X_before, y_before)
            y_pred_before = simple_model.predict(X_before)
            y_pred_after = simple_model.predict(X_after)
        except Exception as e:
            logging.error(f"Simple model error: {str(e)}")
            # If all else fails, use a very basic prediction
            y_pred_before = np.mean(y_before) * np.ones_like(y_before)
            y_pred_after = np.mean(y_before) * np.ones_like(y_after)
    
    # Calculate errors
    errors_before = y_before - y_pred_before
    errors_after = y_after - y_pred_after
    
    # Directly compare error distributions
    mean_shift = abs(np.mean(errors_after) - np.mean(errors_before))
    std_before = np.std(errors_before) if np.std(errors_before) > 0 else 1.0
    mean_shift_normalized = mean_shift / std_before
    
    # Perform KS test
    try:
        ks_statistic, p_value = stats.ks_2samp(errors_before, errors_after)
    except Exception as e:
        logging.error(f"KS test error: {str(e)}")
        ks_statistic, p_value = 0.0, 1.0
    
    # Enhanced drift detection logic
    # 1. Check with drift detector
    drift_detector.set_reference_window(errors_before)
    drift_detected, score, info_dict = drift_detector.detect_drift(errors_after, errors_before)
    
    # 2. Additional direct checks for more sensitivity
    if not drift_detected:
        # Check for significant mean shift
        if mean_shift_normalized > drift_threshold * 0.8:
            drift_detected = True
            logging.info(f"Drift detected by direct mean shift check: {mean_shift_normalized:.4f}")
        
        # Check for significant distribution change
        elif ks_statistic > 0.15 and p_value < significance_level:
            drift_detected = True
            logging.info(f"Drift detected by direct KS test: statistic={ks_statistic:.4f}, p-value={p_value:.6f}")
        
        # Check for variance change
        elif abs(np.std(errors_after) - np.std(errors_before)) / std_before > 0.3:
            drift_detected = True
            logging.info(f"Drift detected by variance change")
    
    return drift_detected, mean_shift_normalized, ks_statistic, p_value

def save_plot(fig, filename, plot_type='general'):
    """
    Save a plot to the appropriate directory based on its type
    
    Args:
        fig: Matplotlib figure to save
        filename: Name of the file
        plot_type: Type of plot (drift, performance, explainability, benchmarks)
    """
    # Create base results directory if it doesn't exist
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True)
    
    # Create subdirectory based on plot type
    if plot_type == 'drift':
        subdir = results_dir / 'drift'
    elif plot_type == 'performance':
        subdir = results_dir / 'performance'
    elif plot_type == 'explainability':
        subdir = results_dir / 'explainability'
    elif plot_type == 'benchmarks':
        subdir = results_dir / 'benchmarks'
    else:
        subdir = results_dir
    
    # Create subdirectory if it doesn't exist
    subdir.mkdir(exist_ok=True, parents=True)
    
    # Save the figure
    fig_path = subdir / filename
    fig.savefig(fig_path)
    plt.close(fig)
    logging.info(f"Saved plot to {fig_path}")
    return fig_path

def run_explainability_analysis(
    args=None, model=None, X=None, y=None, 
    explainer_type='shap', plot_types=None, 
    generate_plots=True, n_samples=5, **kwargs
):
    """
    Run explainability analysis on a trained model
    
    Args:
        args: Command-line arguments
        model: Trained model to explain
        X: Input features
        y: Target values
        explainer_type: Type of explainer to use ('shap', 'lime', 'feature_importance', 'optimizer')
        plot_types: List of plot types to generate
        generate_plots: Whether to generate plots
        n_samples: Number of samples to use for explanation
        **kwargs: Additional parameters for the explainer
    
    Returns:
        Dictionary containing explanation results
    """
    logging.info("Running explainability analysis")
    
    # Process args if provided
    if args is not None:
        explainer_type = args.explainer if hasattr(args, 'explainer') else explainer_type
        generate_plots = args.explain_plots if hasattr(args, 'explain_plots') else generate_plots
        plot_types = args.explain_plot_types if hasattr(args, 'explain_plot_types') else plot_types
        n_samples = args.explain_samples if hasattr(args, 'explain_samples') else n_samples
    
    # Generate synthetic data if not provided
    if X is None or y is None:
        logging.info("No data provided, generating synthetic data...")
        X, y = make_regression(n_samples=200, n_features=10, noise=0.5, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        X, y = X_test, y_test
    
    # Create model if not provided
    if model is None:
        logging.info("No model provided, training a simple model...")
        model = RandomForestRegressor(n_estimators=50, random_state=42)
        model.fit(X_train, y_train)
    
    try:
        from explainability.explainer_factory import ExplainerFactory
        import datetime
        import os
        
        logging.info(f"Running explainability analysis with {explainer_type}")
        
        # Create explainer factory
        explainer_factory = ExplainerFactory()
        
        # Create parameters dictionary based on explainer type
        explainer_params = {}
        
        # Common parameters
        if hasattr(model, 'feature_names_in_'):
            explainer_params['feature_names'] = model.feature_names_in_
        
        # Explainer-specific parameters
        if explainer_type.lower() == 'shap':
            explainer_params['n_samples'] = n_samples
        elif explainer_type.lower() == 'lime':
            explainer_params['n_samples'] = n_samples
            explainer_params['mode'] = 'regression'  # Default to regression
        # No specific parameters for feature_importance or optimizer
        
        # Add any additional parameters
        explainer_params.update(kwargs)
        
        # Create explainer
        explainer = explainer_factory.create_explainer(
            explainer_type, model, **explainer_params
        )
        
        # Generate explanation
        explanation = explainer.explain(X, y, **kwargs)
        
        # Generate plots if requested
        if generate_plots:
            # Get timestamp for filenames
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Get available plot types
            available_plot_types = explainer.supported_plot_types
            logging.info(f"Available plot types for {explainer_type} explainer: {', '.join(available_plot_types)}")
            
            # Validate plot types
            if plot_types:
                valid_plot_types = [pt for pt in plot_types if pt in available_plot_types]
                if not valid_plot_types:
                    logging.warning(f"None of the specified plot types {plot_types} are valid for {explainer_type} explainer. "
                                   f"Valid types are: {available_plot_types}")
                    plot_types = [available_plot_types[0]]  # Default to first available
                else:
                    plot_types = valid_plot_types
            else:
                # Default to first available plot type
                plot_types = [available_plot_types[0]]
            
            # Generate plots
            generated_plots = []
            plot_paths = {}
            
            for plot_type in plot_types:
                try:
                    fig = explainer.plot(plot_type)
                    plot_filename = f"{explainer_type}_{plot_type}_{timestamp}.png"
                    plot_path = save_plot(fig, plot_filename, plot_type='explainability')
                    generated_plots.append(plot_type)
                    plot_paths[plot_type] = str(plot_path)
                except Exception as e:
                    logging.error(f"Error generating {plot_type} plot: {str(e)}")
            
            logging.info(f"Generated plots: {', '.join(generated_plots)}")
            logging.info(f"Plots saved in: results/explainability")
        else:
            plot_paths = {}
        
        # Get feature importance
        feature_importance = explainer.get_feature_importance()
        
        # Return results
        return {
            'explainer_type': explainer_type,
            'feature_importance': feature_importance,
            'explanation': explanation,
            'plot_paths': plot_paths if generate_plots else {}
        }
    
    except Exception as e:
        logging.error(f"Error in run_explainability_analysis: {str(e)}")
        traceback.print_exc()
        return None

def run_optimizer_explainability(optimizer, plot_types=None, generate_plots=True, **kwargs):
    """
    Run explainability analysis on an optimizer
    
    Args:
        optimizer: Optimizer instance to explain
        plot_types: List of plot types to generate
        generate_plots: Whether to generate plots
        **kwargs: Additional parameters for the explainer
        
    Returns:
        Dictionary containing explanation data and plot paths
    """
    try:
        logging.info("Running optimizer explainability analysis")
        
        # Import necessary modules
        from explainability.explainer_factory import ExplainerFactory
        import datetime
        
        # Create explainer
        factory = ExplainerFactory()
        explainer = factory.create_explainer('optimizer', optimizer)
        
        # Generate explanation
        explanation = explainer.explain()
        
        # Generate plots if requested
        if generate_plots and plot_types:
            # Get timestamp for filenames
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # Get available plot types
            available_plot_types = explainer.supported_plot_types
            logging.info(f"Available plot types for optimizer explainer: {', '.join(available_plot_types)}")
            
            # Validate plot types
            if plot_types:
                valid_plot_types = [pt for pt in plot_types if pt in available_plot_types]
                if not valid_plot_types:
                    logging.warning(f"None of the specified plot types {plot_types} are valid for optimizer explainer. "
                                   f"Valid types are: {available_plot_types}")
                    plot_types = [available_plot_types[0]]  # Default to first available
                else:
                    plot_types = valid_plot_types
            else:
                # Default to first available plot type
                plot_types = [available_plot_types[0]]
            
            # Generate plots
            generated_plots = []
            plot_paths = {}
            
            for plot_type in plot_types:
                try:
                    logging.info(f"Generating {plot_type} plot")
                    fig = explainer.plot(plot_type)
                    
                    # Save plot
                    optimizer_name = optimizer.__class__.__name__
                    plot_filename = f"{optimizer_name}_{plot_type}_{timestamp}.png"
                    plot_path = save_plot(fig, plot_filename, plot_type='explainability')
                    generated_plots.append(plot_type)
                    plot_paths[plot_type] = str(plot_path)
                except Exception as e:
                    logging.error(f"Error generating {plot_type} plot: {str(e)}")
            
            logging.info(f"Generated plots: {', '.join(generated_plots)}")
            logging.info(f"Plots saved in: results/explainability")
        else:
            plot_paths = {}
        
        # Add plot paths to explanation
        explanation['plot_paths'] = plot_paths
        explanation['explainer_type'] = 'optimizer'  # Add explainer type
        
        return explanation
    
    except Exception as e:
        logging.error(f"Error in run_optimizer_explainability: {str(e)}")
        traceback.print_exc()
        return None

def run_meta_learning(method='bayesian', surrogate=None, selection=None, exploration=0.2, history_weight=0.7):
    """Run meta-learning process to find the best optimizer for a given problem."""
    logging.info(f"Running meta-learning with method={method}, surrogate={surrogate}, selection={selection}")
    
    # Create directories if they don't exist
    os.makedirs('results/meta_learning', exist_ok=True)
    
    # Define test functions
    test_functions = {
        'sphere': lambda x: np.sum(x**2),
        'rosenbrock': lambda x: np.sum(100.0 * (x[1:] - x[:-1]**2)**2 + (1 - x[:-1])**2),
        'rastrigin': lambda x: 10 * len(x) + np.sum(x**2 - 10 * np.cos(2 * np.pi * x)),
        'ackley': lambda x: -20 * np.exp(-0.2 * np.sqrt(np.sum(x**2) / len(x))) - np.exp(np.sum(np.cos(2 * np.pi * x)) / len(x)) + 20 + np.e
    }
    
    # Initialize optimizers
    dim = 30
    bounds = [(-5, 5)] * dim
    
    # Initialize optimizer wrappers
    aco_opt = AntColonyOptimizer(dim=dim, bounds=bounds)
    gwo_opt = GreyWolfOptimizer(dim=dim, bounds=bounds)
    de_opt = DifferentialEvolutionOptimizer(dim=dim, bounds=bounds)
    es_opt = EvolutionStrategyOptimizer(dim=dim, bounds=bounds)
    de_adaptive_opt = DifferentialEvolutionOptimizer(dim=dim, bounds=bounds, adaptive=True)
    es_adaptive_opt = EvolutionStrategyOptimizer(dim=dim, bounds=bounds, adaptive=True)
    
    # Create dictionary of optimizers
    optimizers = {
        'ACO': aco_opt,
        'GWO': gwo_opt,
        'DE': de_opt,
        'ES': es_opt,
        'DE (Adaptive)': de_adaptive_opt,
        'ES (Adaptive)': es_adaptive_opt
    }
    
    # Initialize meta-optimizer with the correct parameters
    meta_opt = MetaOptimizer(
        dim=dim,
        bounds=bounds,
        optimizers=optimizers,
        n_parallel=2,
        history_file='results/meta_learning/history.json',
        selection_file='results/meta_learning/selection.json'
    )
    
    # Run meta-optimizer on each benchmark function
    results = {}
    best_algorithms = {}
    performance_metrics = {}
    
    for func_name, func in test_functions.items():
        logging.info(f"Running meta-learning on {func_name} function")
        
        # Run meta-optimizer
        meta_opt.reset()  # Reset optimizer state
        
        # Use the correct parameters for optimize
        result = meta_opt.optimize(
            func, 
            max_evals=1000,  # Reduced for quicker results
            context={"function_name": func_name}
        )
        
        # Extract results
        best_score = meta_opt.best_score
        best_solution = meta_opt.best_solution
        history = meta_opt.optimization_history
        
        # Store results
        results[func_name] = {
            'best_score': best_score,
            'best_solution': best_solution,
            'history': history
        }
        
        # Track which optimizer was selected most often
        optimizer_counts = {}
        for entry in history:
            optimizer = entry.get('selected_optimizer', 'unknown')
            optimizer_counts[optimizer] = optimizer_counts.get(optimizer, 0) + 1
        
        # Determine best algorithm
        if optimizer_counts:
            best_algorithm = max(optimizer_counts.items(), key=lambda x: x[1])[0]
        else:
            logging.warning(f"No optimizer selections recorded for {func_name}. Using default.")
            best_algorithm = "default"
        best_algorithms[func_name] = best_algorithm
        
        # Calculate performance metrics
        performance_metrics[func_name] = {
            'best_score': best_score,
            'optimizer_selections': optimizer_counts,
            'convergence_rate': len(history) / 1000  # Simple metric
        }
    
    # Determine overall best algorithm
    all_selections = {}
    for func_name, algorithm in best_algorithms.items():
        all_selections[algorithm] = all_selections.get(algorithm, 0) + 1
    
    if all_selections:
        overall_best_algorithm = max(all_selections.items(), key=lambda x: x[1])[0]
    else:
        logging.warning("No algorithm selections recorded. Using default as best algorithm.")
        overall_best_algorithm = "default"
    
    # Create summary plot
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Plot optimizer selection frequency
    algorithms = list(all_selections.keys())
    frequencies = [all_selections[alg] for alg in algorithms]
    
    ax.bar(algorithms, frequencies)
    ax.set_xlabel('Optimizer')
    ax.set_ylabel('Selection Frequency')
    ax.set_title('Meta-Learner Optimizer Selection Frequency')
    
    # Save plot
    save_plot(fig, 'meta_learner_selection_frequency', plot_type='meta')
    
    return {
        'best_algorithm': overall_best_algorithm,
        'algorithm_selections': best_algorithms,
        'performance': performance_metrics,
        'results': results
    }

def run_evaluation(model=None, X_test=None, y_test=None):
    """
    Evaluate a trained model on test data.
    
    Args:
        model: Trained model to evaluate (if None, creates a default model)
        X_test: Test features (if None, creates synthetic data)
        y_test: Test targets (if None, creates synthetic data)
        
    Returns:
        Dictionary with evaluation results
    """
    logging.info("Running model evaluation")
    
    # Create results directory
    results_dir = Path('results')
    results_dir.mkdir(exist_ok=True, parents=True)
    
    # Create model and data if not provided
    if model is None or X_test is None or y_test is None:
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.datasets import make_regression
        from sklearn.model_selection import train_test_split
        
        # Create synthetic data
        X, y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
        
        # Create and train model
        model = RandomForestRegressor(n_estimators=100, random_state=42)
        model.fit(X_train, y_train)
    
    # Evaluate model
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    r2 = r2_score(y_test, y_pred)
    
    # Create evaluation plot
    fig, ax = plt.subplots(figsize=(10, 6))
    
    # Plot actual vs predicted
    ax.scatter(y_test, y_pred, alpha=0.5)
    
    # Add perfect prediction line
    min_val = min(np.min(y_test), np.min(y_pred))
    max_val = max(np.max(y_test), np.max(y_pred))
    ax.plot([min_val, max_val], [min_val, max_val], 'r--')
    
    ax.set_xlabel('Actual')
    ax.set_ylabel('Predicted')
    ax.set_title('Model Evaluation: Actual vs Predicted')
    
    # Add metrics to plot
    ax.text(0.05, 0.95, f'MSE: {mse:.4f}\nRMSE: {rmse:.4f}\nR²: {r2:.4f}',
            transform=ax.transAxes, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # Save plot
    save_plot(fig, 'model_evaluation', plot_type='evaluation')
    
    return {
        'score': r2,
        'metrics': {
            'mse': mse,
            'rmse': rmse,
            'r2': r2
        },
        'predictions': y_pred.tolist()
    }

def run_optimization_and_evaluation(data_path: str, 
                                  save_dir: str = 'results',
                                  n_runs: int = 30,
                                  max_evals: int = 1000):
    """Run complete optimization and evaluation pipeline with visualizations."""
    
    # Create directories
    os.makedirs(save_dir, exist_ok=True)
    plots_dir = os.path.join(save_dir, 'plots')
    os.makedirs(plots_dir, exist_ok=True)
    
    # Initialize components
    evaluator = FrameworkEvaluator()
    drift_detector = DriftDetector(
        window_size=50,
        drift_threshold=1.8,
        significance_level=0.01
    )
    optimizer_analyzer = OptimizerAnalyzer(optimizers={
        'differential_evolution': DifferentialEvolutionOptimizer,
        'evolution_strategy': EvolutionStrategyOptimizer,
        'ant_colony': AntColonyOptimizer,
        'grey_wolf': GreyWolfOptimizer
    })
    
    # Load and preprocess data
    data = pd.read_csv(data_path)
    X = data.drop('target', axis=1)
    y = data['target']
    
    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    
    # Run optimization and get best model
    best_model = run_optimization(args=None, X_train=X_train, y_train=y_train, n_runs=n_runs, max_evals=max_evals)
    
    # Generate predictions
    y_pred = best_model.predict(X_test)
    y_prob = best_model.predict_proba(X_test)[:, 1]
    
    # Track performance and drift
    for i in range(len(X_test)):
        # Evaluate prediction performance
        evaluator.evaluate_prediction_performance(
            y_test[i:i+1], 
            y_pred[i:i+1], 
            y_prob[i:i+1]
        )
        
        # Track feature importance
        evaluator.track_feature_importance(
            X.columns, 
            best_model.feature_importances_
        )
        
        # Detect drift
        drift_detected = drift_detector.detect_drift(
            y_test[i:i+1],
            y_pred[i:i+1]
        )
        if drift_detected:
            evaluator.track_drift_event(drift_detector.get_statistics())
    
    # Generate visualizations
    
    # 1. Drift Detection Results
    drift_detector.plot_detection_results(
        save_path=os.path.join(plots_dir, 'drift_detection_results.png')
    )
    
    # 2. Drift Analysis
    drift_detector.plot_drift_analysis(
        save_path=os.path.join(plots_dir, 'drift_analysis.png')
    )
    drift_detector.save_analysis(plots_dir)  # Saves additional drift plots
    
    # 3. Framework Performance
    evaluator.plot_framework_performance(
        save_path=os.path.join(plots_dir, 'framework_performance.png')
    )
    
    # 4. Pipeline Performance
    evaluator.plot_pipeline_performance(
        save_path=os.path.join(plots_dir, 'pipeline_performance.png')
    )
    
    # 5. Performance Boxplot
    evaluator.plot_performance_boxplot(
        save_path=os.path.join(plots_dir, 'performance_boxplot.png')
    )
    
    # 6. Model Evaluation
    plot_model_evaluation(
        y_test, y_pred, y_prob,
        save_path=os.path.join(plots_dir, 'model_evaluation.png')
    )
    
    # 7. Optimizer Analysis
    optimizer_analyzer.plot_landscape_analysis(
        save_path=os.path.join(plots_dir, 'optimizer_landscape.png')
    )
    optimizer_analyzer.plot_gradient_analysis(
        save_path=os.path.join(plots_dir, 'optimizer_gradient.png')
    )
    optimizer_analyzer.plot_parameter_adaptation(
        save_path=os.path.join(plots_dir, 'optimizer_parameters.png')
    )
    
    # Save model
    model_path = os.path.join(save_dir, 'best_model.pkl')
    with open(model_path, 'wb') as f:
        pickle.dump(best_model, f)
        
    # Generate summary report
    with open(os.path.join(save_dir, 'optimization_report.txt'), 'w') as f:
        f.write("Optimization and Evaluation Report\n")
        f.write("================================\n\n")
        
        # Model Performance
        f.write("Model Performance\n")
        f.write("-----------------\n")
        metrics = evaluator.generate_performance_report()
        for metric, value in metrics.items():
            f.write(f"{metric}: {value:.4f}\n")
        f.write("\n")
        
        # Drift Analysis
        f.write("Drift Analysis\n")
        f.write("--------------\n")
        drift_stats = drift_detector.get_statistics()
        for stat, value in drift_stats.items():
            f.write(f"{stat}: {value:.4f}\n")
        f.write("\n")
        
        # Optimization Summary
        f.write("Optimization Summary\n")
        f.write("-------------------\n")
        opt_stats = optimizer_analyzer.get_statistics() if hasattr(optimizer_analyzer, 'get_statistics') else {}
        for stat, value in opt_stats.items():
            f.write(f"{stat}: {value}\n")
            
    return {
        'model': best_model,
        'evaluator': evaluator,
        'drift_detector': drift_detector,
        'optimizer_analyzer': optimizer_analyzer,
        'performance_metrics': evaluator.generate_performance_report()
    }

def run_drift_detection(args):
    """
    Run drift detection on synthetic data.
    
    Args:
        args: Command-line arguments
    """
    logging.info("Running drift detection")
    
    # Get parameters from args
    window_size = args.drift_window if hasattr(args, 'drift_window') else 50
    drift_threshold = args.drift_threshold if hasattr(args, 'drift_threshold') else 0.5
    significance_level = args.drift_significance if hasattr(args, 'drift_significance') else 0.05
    visualize = args.visualize if hasattr(args, 'visualize') else False
    
    # Generate synthetic data with drift
    n_samples = 1000
    n_features = 10
    drift_points = [300, 600]  # Drift at these points
    
    X, y, _ = generate_synthetic_data_with_drift(
        n_samples=n_samples, 
        n_features=n_features, 
        drift_points=drift_points
    )
    
    # Set up the drift detector
    detector = DriftDetector(
        window_size=window_size,
        drift_threshold=drift_threshold,
        significance_level=significance_level
    )
    
    # Monitor for drift
    drift_points_detected = []
    for i in range(window_size, len(X)):
        X_window = X[i-window_size:i]
        y_window = y[i-window_size:i]
        
        is_drift, score, info_dict = detector.detect_drift(X_window, y_window)
        
        if is_drift:
            drift_points_detected.append(i)
            p_value = info_dict['p_value']
            logging.info(f"Drift detected at point {i}, score: {score:.4f}, p-value: {p_value:.4f}")
    
    # Visualize results if requested
    if visualize:
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.scatter(range(len(y)), y, alpha=0.6, label='Data points')
        
        # Mark true drift points
        for point in drift_points:
            ax.axvline(x=point, color='r', linestyle='--', label='True drift' if point == drift_points[0] else None)
        
        # Mark detected drift points
        for point in drift_points_detected:
            ax.axvline(x=point, color='g', linestyle='-', label='Detected drift' if point == drift_points_detected[0] else None)
        
        ax.set_title('Drift Detection Results')
        ax.set_xlabel('Time steps')
        ax.set_ylabel('Target values')
        ax.legend()
        
        # Save plot
        save_plot(fig, 'drift_detection_results', plot_type='drift')
    
    # Return the results
    results = {
        'true_drift_points': drift_points,
        'detected_drift_points': drift_points_detected,
        'window_size': window_size,
        'drift_threshold': drift_threshold,
        'significance_level': significance_level
    }
    
    return results

def run_meta_learner_with_drift_detection(args):
    """
    Run meta-learner with drift detection.
    
    Args:
        args: Command-line arguments
    """
    logging.info("Running meta-learner with drift detection")
    
    # Get parameters from args
    window_size = args.drift_window if hasattr(args, 'drift_window') else 50
    drift_threshold = args.drift_threshold if hasattr(args, 'drift_threshold') else 0.5
    significance_level = args.drift_significance if hasattr(args, 'drift_significance') else 0.05
    visualize = args.visualize if hasattr(args, 'visualize') else False
    
    # Generate synthetic data with drift
    n_samples = 1000
    n_features = 10
    drift_points = [300, 600]  # Drift at these points
    
    X, y, _ = generate_synthetic_data_with_drift(
        n_samples=n_samples, 
        n_features=n_features, 
        drift_points=drift_points
    )
    
    # Set up the drift detector
    detector = DriftDetector(
        window_size=window_size,
        drift_threshold=drift_threshold,
        significance_level=significance_level
    )
    
    # Set up the meta-learner
    meta_learner = MetaLearner(
        method='bayesian',
        surrogate_model=None,
        selection_strategy='bandit',
        exploration_factor=0.2,
        history_weight=0.7
    )
    
    # Add some algorithms to the meta-learner
    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    from sklearn.linear_model import LinearRegression
    
    # Create simple algorithm wrappers
    class Algorithm:
        def __init__(self, name, model):
            self.name = name
            self.model = model
            self.best_config = None
            self.best_score = float('-inf')
            self.is_fitted = False
            
        def fit(self, X, y):
            self.model.fit(X, y)
            self.is_fitted = True
            
        def predict(self, X):
            if not self.is_fitted:
                logging.warning(f"{self.name} model is not fitted yet. Fitting with dummy data before prediction.")
                # Create dummy data to fit the model
                import numpy as np
                dummy_X = np.random.random((10, X.shape[1]))
                dummy_y = np.random.random(10)
                self.fit(dummy_X, dummy_y)
                logging.warning(f"{self.name} model fitted with dummy data. Predictions may not be meaningful.")
            return self.model.predict(X)
            
        def suggest(self, num_points=1):
            """Suggest next points to evaluate (dummy implementation)"""
            import numpy as np
            # Check if we should return a single point or multiple
            if num_points == 1:
                return np.random.uniform(-1, 1, (1, 10))  # Return as 2D array with 1 row
            # Otherwise return multiple points
            return np.random.uniform(-1, 1, (num_points, 10))
            
        def evaluate(self, X):
            """Evaluate configurations (dummy implementation)"""
            # Return dummy scores for the points
            import numpy as np
            # Make sure X is 2D when computing shape
            if isinstance(X, np.ndarray):
                if X.ndim == 1:
                    X = X.reshape(1, -1)
                # Ensure we return 1D array for multiple points or scalar for single point
                if X.shape[0] > 1:
                    return np.random.uniform(0, 1, X.shape[0]).astype(float)
                else:
                    return float(np.random.uniform(0, 1))
            else:
                # Handle scalar case
                return float(np.random.uniform(0, 1))
            
        def update(self, X, scores=None):
            """Update the optimizer with new data (dummy implementation)"""
            import numpy as np
            
            # Always treat X and scores as arrays for consistency
            try:
                # Handle scalar X by converting to a 1x1 array
                if np.isscalar(X):
                    X = np.array([[float(X)]])
                elif isinstance(X, np.ndarray):
                    if X.ndim == 0:  # Handle numpy scalars
                        X = np.array([[float(X)]])
                    elif X.ndim == 1:
                        X = X.reshape(1, -1)
                        
                # Just record best score if scores are provided
                if scores is not None:
                    # Ensure scores is a float array
                    if np.isscalar(scores):
                        scores = np.array([float(scores)])
                    else:
                        scores = np.asarray(scores, dtype=float)
                        if scores.ndim == 0:  # scalar
                            scores = np.array([float(scores)])
                        
                    if scores.size > 0:
                        # Ensure best_score is initialized
                        if self.best_score == float('-inf'):
                            self.best_score = float(scores.max())
                            best_idx = scores.argmax() if scores.size > 1 else 0
                            self.best_config = X[best_idx].copy() if best_idx < len(X) else X[0].copy()
                        elif scores.max() > self.best_score:
                            self.best_score = float(scores.max())
                            best_idx = scores.argmax() if scores.size > 1 else 0
                            self.best_config = X[best_idx].copy() if best_idx < len(X) else X[0].copy()
                # Otherwise fit the model with new data
                else:
                    self.fit(X, scores)
            except Exception as e:
                logging.error(f"Error in update: {str(e)}")
                # Fallback to prevent crashes
                if self.best_score == float('-inf'):
                    self.best_score = 0.0
                    if isinstance(X, np.ndarray) and X.size > 0:
                        self.best_config = X[0].copy() if X.ndim > 1 else X.copy()
            
        def get_best(self):
            """Get best configuration found so far"""
            return self.best_config, self.best_score
        
    # Add algorithms to meta-learner
    meta_learner.algorithms = [
        Algorithm("RF", RandomForestRegressor(n_estimators=100, random_state=42)),
        Algorithm("GB", GradientBoostingRegressor(n_estimators=100, random_state=42)),
        Algorithm("LR", LinearRegression())
    ]
    
    # Set the drift detector for the meta-learner
    meta_learner.drift_detector = detector
    
    # Initialize performance metrics
    mse_history = []
    drift_detected_points = []
    
    # Run online learning with drift detection
    batch_size = 50
    for i in range(0, len(X) - batch_size, batch_size):
        # Get current batch
        X_batch = X[i:i+batch_size]
        y_batch = y[i:i+batch_size]
        
        # Train the meta-learner on the current batch
        meta_learner.update(X_batch, y_batch)
        
        # Make predictions
        y_pred = meta_learner.predict(X_batch)
        mse = mean_squared_error(y_batch, y_pred)
        mse_history.append(mse)
        
        # Check for drift
        if i >= window_size:
            X_window = X[i-window_size:i]
            y_window = y[i-window_size:i]
            
            is_drift, drift_score, info_dict = detector.detect_drift(X_window, y_window)
            p_value = info_dict['p_value']
            
            if is_drift:
                drift_detected_points.append(i)
                logging.info(f"Drift detected at point {i}, score: {drift_score:.4f}, p-value: {p_value:.4f}")
                
                # Reset the meta-learner when drift is detected
                meta_learner.reset_weights()
    
    # Visualize results if requested
    if visualize:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)
        
        # Plot target values and drift points
        ax1.scatter(range(len(y)), y, alpha=0.6, s=5, label='Data points')
        
        # Mark true drift points
        for point in drift_points:
            ax1.axvline(x=point, color='r', linestyle='--', label='True drift' if point == drift_points[0] else None)
        
        # Mark detected drift points
        for point in drift_detected_points:
            ax1.axvline(x=point, color='g', linestyle='-', label='Detected drift' if point == drift_detected_points[0] else None)
        
        ax1.set_title('Data Stream with Drift')
        ax1.set_ylabel('Target values')
        ax1.legend()
        
        # Plot MSE history
        indices = range(0, len(X) - batch_size, batch_size)
        ax2.plot(indices, mse_history, label='MSE')
        
        # Mark true drift points
        for point in drift_points:
            ax2.axvline(x=point, color='r', linestyle='--')
        
        # Mark detected drift points
        for point in drift_detected_points:
            ax2.axvline(x=point, color='g', linestyle='-')
            
        ax2.set_title('MSE History')
        ax2.set_xlabel('Time steps')
        ax2.set_ylabel('Mean Squared Error')
        
        plt.tight_layout()
        
        # Save plot
        save_plot(fig, 'meta_learner_with_drift_detection', plot_type='drift')
    
    # Return the results
    results = {
        'true_drift_points': drift_points,
        'detected_drift_points': drift_detected_points,
        'mse_history': mse_history,
        'window_size': window_size,
        'drift_threshold': drift_threshold,
        'significance_level': significance_level
    }
    
    return results

def explain_drift(args):
    """
    Explain drift when detected.
    
    Args:
        args: Command-line arguments
    """
    logging.info("Explaining drift in the data")
    
    # Get parameters from args
    window_size = args.drift_window if hasattr(args, 'drift_window') else 50
    drift_threshold = args.drift_threshold if hasattr(args, 'drift_threshold') else 0.5
    significance_level = args.drift_significance if hasattr(args, 'drift_significance') else 0.05
    visualize = args.visualize if hasattr(args, 'visualize') else False
    
    # Generate synthetic data with drift
    n_samples = 1000
    n_features = 10
    drift_points = [300, 600]  # Drift at these points
    
    X, y, drift_types = generate_synthetic_data_with_drift(
        n_samples=n_samples,
        n_features=n_features,
        drift_points=drift_points
    )
    
    # Set up the drift detector
    detector = DriftDetector(
        window_size=window_size,
        drift_threshold=drift_threshold,
        significance_level=significance_level
    )
    
    # Monitor for drift
    drift_points_detected = []
    drift_scores = []
    p_values = []
    feature_contributions = []
    
    for i in range(window_size, len(X)):
        X_window = X[i-window_size:i]
        y_window = y[i-window_size:i]
        
        is_drift, drift_score, info_dict = detector.detect_drift(X_window, y_window)
        p_value = info_dict['p_value']
        drift_scores.append(drift_score)
        p_values.append(p_value)
        
        if is_drift:
            drift_points_detected.append(i)
            logging.info(f"Drift detected at point {i}, score: {drift_score:.4f}, p-value: {p_value:.4f}")
            
            # Calculate feature contributions to drift
            before_drift = X[i-window_size:i-window_size//2]
            after_drift = X[i-window_size//2:i]
            
            # Calculate mean shift for each feature
            mean_before = np.mean(before_drift, axis=0)
            mean_after = np.mean(after_drift, axis=0)
            mean_shift = np.abs(mean_after - mean_before)
            
            # Calculate variance shift for each feature
            var_before = np.var(before_drift, axis=0)
            var_after = np.var(after_drift, axis=0)
            var_shift = np.abs(var_after - var_before)
            
            # Calculate distribution shift using KS test
            ks_pvalues = []
            for f in range(n_features):
                _, ks_pvalue = stats.ks_2samp(before_drift[:, f], after_drift[:, f])
                ks_pvalues.append(1 - ks_pvalue)  # Convert p-value to a "contribution" score
            
            # Combine the metrics (mean shift, variance shift, KS test)
            feature_contrib = (mean_shift / np.max(mean_shift) + 
                              var_shift / np.max(var_shift) + 
                              np.array(ks_pvalues)) / 3
            
            feature_contributions.append((i, feature_contrib))
    
    # Visualize results if requested
    if visualize:
        # Create a figure with 4 subplots
        fig, axs = plt.subplots(4, 1, figsize=(12, 16), sharex=True)
        
        # Plot 1: Target values and drift points
        axs[0].scatter(range(len(y)), y, alpha=0.6, s=5, label='Data points')
        
        # Mark true drift points
        for point in drift_points:
            axs[0].axvline(x=point, color='r', linestyle='--', 
                           label='True drift' if point == drift_points[0] else None)
        
        # Mark detected drift points
        for point in drift_points_detected:
            axs[0].axvline(x=point, color='g', linestyle='-', 
                           label='Detected drift' if point == drift_points_detected[0] else None)
        
        axs[0].set_title('Data Stream with Drift')
        axs[0].set_ylabel('Target values')
        axs[0].legend()
        
        # Plot 2: Drift scores
        axs[1].plot(range(window_size, len(X)), drift_scores, label='Drift score')
        axs[1].axhline(y=drift_threshold, color='r', linestyle='--', label='Threshold')
        axs[1].set_title('Drift Scores')
        axs[1].set_ylabel('Score')
        axs[1].legend()
        
        # Plot 3: P-values
        axs[2].plot(range(window_size, len(X)), p_values, label='P-value')
        axs[2].axhline(y=significance_level, color='r', linestyle='--', label='Significance level')
        axs[2].set_title('Statistical Significance (P-value)')
        axs[2].set_ylabel('P-value')
        axs[2].legend()
        
        # Plot 4: Feature contributions
        if feature_contributions:
            # Get the contribution at the first detected drift point
            drift_point, contributions = feature_contributions[0]
            
            # Plot feature contributions as a bar chart
            axs[3].bar(range(n_features), contributions)
            axs[3].set_title(f'Feature Contributions to Drift at Point {drift_point}')
            axs[3].set_xlabel('Feature Index')
            axs[3].set_ylabel('Contribution Score')
            axs[3].set_xticks(range(n_features))
        
        plt.tight_layout()
        
        # Save plot
        save_plot(fig, 'drift_explanation', plot_type='drift')
    
    # Print drift explanation
    print("\nDrift Explanation:")
    print("-----------------")
    print(f"True drift points: {drift_points}")
    print(f"Detected drift points: {drift_points_detected}")
    
    if feature_contributions:
        print("\nFeature Contributions to Drift:")
        for i, (point, contributions) in enumerate(feature_contributions):
            print(f"\nDrift Point {i+1} (at index {point}):")
            
            # Sort features by contribution
            feature_indices = np.argsort(contributions)[::-1]
            
            for rank, idx in enumerate(feature_indices):
                print(f"  Rank {rank+1}: Feature {idx} (contribution: {contributions[idx]:.4f})")
            
            # Determine the type of drift if available
            if drift_types and point in drift_types:
                print(f"  Drift type: {drift_types[point]}")
    
    # Return the results
    results = {
        'true_drift_points': drift_points,
        'detected_drift_points': drift_points_detected,
        'drift_scores': drift_scores,
        'p_values': p_values,
        'feature_contributions': feature_contributions,
        'window_size': window_size,
        'drift_threshold': drift_threshold,
        'significance_level': significance_level
    }
    
    return results

def run_migraine_data_import(args):
    """
    Import new migraine data with potentially different schema.
    
    Args:
        args: Command-line arguments containing data path, output path, and options
    
    Returns:
        Dictionary with results of the import operation
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Importing migraine data from {args.data_path}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Import the data
        imported_data = predictor.import_data(
            data_path=args.data_path,
            file_format=args.file_format,
            add_new_columns=args.add_new_columns
        )
        
        # If requested, add derived features
        if args.derived_features:
            for feature_def in args.derived_features:
                name, formula = feature_def.split(":", 1)
                predictor.add_derived_feature(name=name, formula=formula)
                logging.info(f"Added derived feature: {name} with formula: {formula}")
        
        # If requested, train a model with the imported data
        if args.train_model:
            model_id = predictor.train(
                data=imported_data,
                model_name=args.model_name or "migraine_model",
                description=args.model_description or f"Model trained with data from {args.data_path}",
                make_default=args.make_default
            )
            logging.info(f"Trained model with ID: {model_id}")
            model_info = {"model_id": model_id}
        else:
            model_info = {}
        
        # Save the imported data if requested
        if args.save_processed_data:
            output_path = os.path.join(args.data_dir, "processed_data.csv")
            imported_data.to_csv(output_path, index=False)
            logging.info(f"Saved processed data to {output_path}")
        
        # Get schema information
        schema_info = predictor.get_schema_info()
        
        # If summary is requested, print it
        if args.summary:
            print("
Migraine Data Import Summary:")
            print(f"Imported data shape: {imported_data.shape}")
            print(f"New columns added: {len(schema_info['optional_features'])}")
            print(f"Derived features created: {len(schema_info['derived_features'])}")
            if args.train_model:
                print(f"Model trained with ID: {model_id}")
            print(f"Feature columns: {predictor.feature_columns}")
        
        return {
            "success": True,
            "data_shape": imported_data.shape,
            "schema_info": schema_info,
            "model_info": model_info
        }
        
    except Exception as e:
        logging.error(f"Error importing migraine data: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def run_migraine_prediction(args):
    """
    Run prediction using a migraine model on new data, handling missing features.
    
    Args:
        args: Command-line arguments containing prediction parameters
        
    Returns:
        Dictionary with prediction results
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Running migraine prediction using data from {args.prediction_data}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Load the model if specified
        if args.model_id:
            predictor.load_model(args.model_id)
            logging.info(f"Loaded model with ID: {args.model_id}")
        
        # Import prediction data
        pred_data = predictor.import_data(
            data_path=args.prediction_data,
            add_new_columns=False  # Don't add new columns for prediction
        )
        
        # Make predictions with missing features
        predictions = predictor.predict_with_missing_features(pred_data)
        
        # Format results
        results = []
        for i, pred in enumerate(predictions):
            sample_result = {
                "index": i,
                "prediction": pred["prediction"],
                "probability": pred["probability"],
                "top_features": sorted(
                    pred["feature_importances"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5] if "feature_importances" in pred else []
            }
            results.append(sample_result)
        
        # Save results if requested
        if args.save_predictions:
            # Create DataFrame with predictions
            pred_df = pd.DataFrame({
                "prediction": [p["prediction"] for p in predictions],
                "probability": [p["probability"] for p in predictions]
            })
            # Add original data
            pred_df = pd.concat([pred_data.reset_index(drop=True), pred_df], axis=1)
            output_path = os.path.join(args.data_dir, "predictions.csv")
            pred_df.to_csv(output_path, index=False)
            logging.info(f"Saved predictions to {output_path}")
        
        # If summary is requested, print it
        if args.summary:
            print("
Migraine Prediction Summary:")
            print(f"Number of samples predicted: {len(predictions)}")
            migraine_count = sum(1 for p in predictions if p["prediction"] == 1)
            print(f"Predicted migraines: {migraine_count} ({migraine_count/len(predictions)*100:.2f}%)")
            print(f"Average probability: {sum(p['probability'] for p in predictions)/len(predictions):.4f}")
            print("
Sample predictions:")
            for i, result in enumerate(results[:5]):  # Show first 5 predictions
                print(f"Sample {i}: {'Migraine' if result['prediction'] == 1 else 'No Migraine'} " +
                      f"(Probability: {result['probability']:.4f})")
        
        return {
            "success": True,
            "predictions": results,
            "summary": {
                "total_samples": len(predictions),
                "predicted_migraines": sum(1 for p in predictions if p["prediction"] == 1),
                "average_probability": sum(p["probability"] for p in predictions)/len(predictions)
            }
        }
        
    except Exception as e:
        logging.error(f"Error running migraine prediction: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def run_migraine_data_import(args):
    """
    Import new migraine data with potentially different schema.
    
    Args:
        args: Command-line arguments containing data path, output path, and options
    
    Returns:
        Dictionary with results of the import operation
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Importing migraine data from {args.data_path}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Import the data
        imported_data = predictor.import_data(
            data_path=args.data_path,
            file_format=args.file_format,
            add_new_columns=args.add_new_columns
        )
        
        # If requested, add derived features
        if args.derived_features:
            for feature_def in args.derived_features:
                name, formula = feature_def.split(":", 1)
                predictor.add_derived_feature(name=name, formula=formula)
                logging.info(f"Added derived feature: {name} with formula: {formula}")
        
        # If requested, train a model with the imported data
        if args.train_model:
            model_id = predictor.train(
                data=imported_data,
                model_name=args.model_name or "migraine_model",
                description=args.model_description or f"Model trained with data from {args.data_path}",
                make_default=args.make_default
            )
            logging.info(f"Trained model with ID: {model_id}")
            model_info = {"model_id": model_id}
        else:
            model_info = {}
        
        # Save the imported data if requested
        if args.save_processed_data:
            output_path = os.path.join(args.data_dir, "processed_data.csv")
            imported_data.to_csv(output_path, index=False)
            logging.info(f"Saved processed data to {output_path}")
        
        # Get schema information
        schema_info = predictor.get_schema_info()
        
        # If summary is requested, print it
        if args.summary:
            print("
Migraine Data Import Summary:")
            print(f"Imported data shape: {imported_data.shape}")
            print(f"New columns added: {len(schema_info['optional_features'])}")
            print(f"Derived features created: {len(schema_info['derived_features'])}")
            if args.train_model:
                print(f"Model trained with ID: {model_id}")
            print(f"Feature columns: {predictor.feature_columns}")
        
        return {
            "success": True,
            "data_shape": imported_data.shape,
            "schema_info": schema_info,
            "model_info": model_info
        }
        
    except Exception as e:
        logging.error(f"Error importing migraine data: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def run_migraine_prediction(args):
    """
    Run prediction using a migraine model on new data, handling missing features.
    
    Args:
        args: Command-line arguments containing prediction parameters
        
    Returns:
        Dictionary with prediction results
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Running migraine prediction using data from {args.prediction_data}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Load the model if specified
        if args.model_id:
            predictor.load_model(args.model_id)
            logging.info(f"Loaded model with ID: {args.model_id}")
        
        # Import prediction data
        pred_data = predictor.import_data(
            data_path=args.prediction_data,
            add_new_columns=False  # Don't add new columns for prediction
        )
        
        # Make predictions with missing features
        predictions = predictor.predict_with_missing_features(pred_data)
        
        # Format results
        results = []
        for i, pred in enumerate(predictions):
            sample_result = {
                "index": i,
                "prediction": pred["prediction"],
                "probability": pred["probability"],
                "top_features": sorted(
                    pred["feature_importances"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5] if "feature_importances" in pred else []
            }
            results.append(sample_result)
        
        # Save results if requested
        if args.save_predictions:
            # Create DataFrame with predictions
            pred_df = pd.DataFrame({
                "prediction": [p["prediction"] for p in predictions],
                "probability": [p["probability"] for p in predictions]
            })
            # Add original data
            pred_df = pd.concat([pred_data.reset_index(drop=True), pred_df], axis=1)
            output_path = os.path.join(args.data_dir, "predictions.csv")
            pred_df.to_csv(output_path, index=False)
            logging.info(f"Saved predictions to {output_path}")
        
        # If summary is requested, print it
        if args.summary:
            print("
Migraine Prediction Summary:")
            print(f"Number of samples predicted: {len(predictions)}")
            migraine_count = sum(1 for p in predictions if p["prediction"] == 1)
            print(f"Predicted migraines: {migraine_count} ({migraine_count/len(predictions)*100:.2f}%)")
            print(f"Average probability: {sum(p['probability'] for p in predictions)/len(predictions):.4f}")
            print("
Sample predictions:")
            for i, result in enumerate(results[:5]):  # Show first 5 predictions
                print(f"Sample {i}: {'Migraine' if result['prediction'] == 1 else 'No Migraine'} " +
                      f"(Probability: {result['probability']:.4f})")
        
        return {
            "success": True,
            "predictions": results,
            "summary": {
                "total_samples": len(predictions),
                "predicted_migraines": sum(1 for p in predictions if p["prediction"] == 1),
                "average_probability": sum(p["probability"] for p in predictions)/len(predictions)
            }
        }
        
    except Exception as e:
        logging.error(f"Error running migraine prediction: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def parse_args():
    """
    Parse command-line arguments.
    """
    parser = argparse.ArgumentParser(description='Run optimization framework')
    parser.add_argument('--config', type=str, help='Path to configuration file')
    parser.add_argument('--optimize', action='store_true', help='Run optimization')
    parser.add_argument('--evaluate', action='store_true', help='Evaluate model')
    parser.add_argument('--meta', action='store_true', help='Run meta-learning')
    parser.add_argument('--drift', action='store_true', help='Run drift detection')
    parser.add_argument('--run-meta-learner-with-drift', action='store_true', help='Run meta-learner with drift detection')
    parser.add_argument('--explain-drift', action='store_true', help='Explain drift when detected')
    
    # Explainability arguments
    parser.add_argument('--explain', action='store_true', help='Run explainability analysis')
    parser.add_argument('--explainer', type=str, default='shap', choices=['shap', 'lime', 'feature_importance', 'optimizer'], 
                        help='Explainer type to use')
    parser.add_argument('--explain-plots', action='store_true', help='Generate and save explainability plots')
    parser.add_argument('--explain-plot-types', type=str, nargs='+', 
                        help='Specific plot types to generate (e.g., summary waterfall force dependence)')
    parser.add_argument('--explain-samples', type=int, default=50, help='Number of samples to use for explainability')
    
    # Meta-learner and optimization parameters
    parser.add_argument('--method', type=str, default='bayesian', help='Method for meta-learner')
    parser.add_argument('--surrogate', type=str, default=None, help='Surrogate model for meta-learner')
    parser.add_argument('--selection', type=str, default=None, help='Selection strategy for meta-learner')
    parser.add_argument('--exploration', type=float, default=0.2, help='Exploration factor for meta-learner')
    parser.add_argument('--history', type=float, default=0.7, help='History weight for meta-learner')
    
    # Drift detection parameters
    parser.add_argument('--drift-window', type=int, default=50, help='Window size for drift detection')
    parser.add_argument('--drift-threshold', type=float, default=0.5, help='Threshold for drift detection')
    parser.add_argument('--drift-significance', type=float, default=0.05, help='Significance level for drift detection')
    
    # Migraine data import parameters
    parser.add_argument('--import-migraine-data', action='store_true', help='Import new migraine data')
    parser.add_argument('--data-path', type=str, help='Path to migraine data file')
    parser.add_argument('--data-dir', type=str, default='data', help='Directory to store data files')
    parser.add_argument('--model-dir', type=str, default='models', help='Directory to store model files')
    parser.add_argument('--file-format', type=str, default='csv', choices=['csv', 'excel', 'json', 'parquet'], 
                        help='Format of the input data file')
    parser.add_argument('--add-new-columns', action='store_true', help='Add new columns found in the data to the schema')
    parser.add_argument('--derived-features', type=str, nargs='+', 
                        help='Derived features to create (format: "name:formula")')
    parser.add_argument('--train-model', action='store_true', help='Train a model with the imported data')
    parser.add_argument('--model-name', type=str, help='Name for the trained model')
    parser.add_argument('--model-description', type=str, help='Description for the trained model')
    parser.add_argument('--make-default', action='store_true', help='Make the trained model the default')
    parser.add_argument('--save-processed-data', action='store_true', help='Save the processed data')
    
    # Migraine prediction parameters
    parser.add_argument('--predict-migraine', action='store_true', help='Run migraine prediction')
    parser.add_argument('--prediction-data', type=str, help='Path to data for prediction')
    parser.add_argument('--model-id', type=str, help='ID of the model to use for prediction')
    parser.add_argument('--save-predictions', action='store_true', help='Save prediction results')
    
    # Visualization and summary options
    parser.add_argument('--visualize', action='store_true', help='Visualize results')
    parser.add_argument('--summary', action='store_true', help='Print summary of results')
    
    return parser.parse_args()

def main():
    """
    Main entry point for the CLI.
    """
    args = parse_args()
    
    if args.optimize:
        run_optimization(args)
    
    if args.evaluate:
        run_evaluation(args)
    
    if args.meta:
        run_meta_learning(args)
    
    if args.drift:
        run_drift_detection(args)
    
    if args.explain:
        run_explainability_analysis(args)
    
    if args.run_meta_learner_with_drift:
        run_meta_learner_with_drift_detection(args)
    
    if args.explain_drift:
        explain_drift(args)
        
    if args.import_migraine_data:
        import_results = run_migraine_data_import(args)
        if not import_results["success"]:
            logging.error(f"Migraine data import failed: {import_results.get('error', 'Unknown error')}")
        
    if args.predict_migraine:
        prediction_results = run_migraine_prediction(args)
        if not prediction_results["success"]:
            logging.error(f"Migraine prediction failed: {prediction_results.get('error', 'Unknown error')}")

    print("All operations completed successfully.")


def parse_args():
    """
    Parse command-line arguments.
    """
    parser = argparse.ArgumentParser(description='Run optimization framework')
    parser.add_argument('--config', type=str, help='Path to configuration file')
    parser.add_argument('--optimize', action='store_true', help='Run optimization')
    parser.add_argument('--evaluate', action='store_true', help='Evaluate model')
    parser.add_argument('--meta', action='store_true', help='Run meta-learning')
    parser.add_argument('--drift', action='store_true', help='Run drift detection')
    parser.add_argument('--run-meta-learner-with-drift', action='store_true', help='Run meta-learner with drift detection')
    parser.add_argument('--explain-drift', action='store_true', help='Explain drift when detected')
    
    # Explainability arguments
    parser.add_argument('--explain', action='store_true', help='Run explainability analysis')
    parser.add_argument('--explainer', type=str, default='shap', choices=['shap', 'lime', 'feature_importance', 'optimizer'], 
                        help='Explainer type to use')
    parser.add_argument('--explain-plots', action='store_true', help='Generate and save explainability plots')
    parser.add_argument('--explain-plot-types', type=str, nargs='+', 
                        help='Specific plot types to generate (e.g., summary waterfall force dependence)')
    parser.add_argument('--explain-samples', type=int, default=50, help='Number of samples to use for explainability')
    
    # Meta-learner and optimization parameters
    parser.add_argument('--method', type=str, default='bayesian', help='Method for meta-learner')
    parser.add_argument('--surrogate', type=str, default=None, help='Surrogate model for meta-learner')
    parser.add_argument('--selection', type=str, default=None, help='Selection strategy for meta-learner')
    parser.add_argument('--exploration', type=float, default=0.2, help='Exploration factor for meta-learner')
    parser.add_argument('--history', type=float, default=0.7, help='History weight for meta-learner')
    
    # Drift detection parameters
    parser.add_argument('--drift-window', type=int, default=50, help='Window size for drift detection')
    parser.add_argument('--drift-threshold', type=float, default=0.5, help='Threshold for drift detection')
    parser.add_argument('--drift-significance', type=float, default=0.05, help='Significance level for drift detection')
    
    # Migraine data import parameters
    parser.add_argument('--import-migraine-data', action='store_true', help='Import new migraine data')
    parser.add_argument('--data-path', type=str, help='Path to migraine data file')
    parser.add_argument('--data-dir', type=str, default='data', help='Directory to store data files')
    parser.add_argument('--model-dir', type=str, default='models', help='Directory to store model files')
    parser.add_argument('--file-format', type=str, default='csv', choices=['csv', 'excel', 'json', 'parquet'], 
                        help='Format of the input data file')
    parser.add_argument('--add-new-columns', action='store_true', help='Add new columns found in the data to the schema')
    parser.add_argument('--derived-features', type=str, nargs='+', 
                        help='Derived features to create (format: "name:formula")')
    parser.add_argument('--train-model', action='store_true', help='Train a model with the imported data')
    parser.add_argument('--model-name', type=str, help='Name for the trained model')
    parser.add_argument('--model-description', type=str, help='Description for the trained model')
    parser.add_argument('--make-default', action='store_true', help='Make the trained model the default')
    parser.add_argument('--save-processed-data', action='store_true', help='Save the processed data')
    
    # Migraine prediction parameters
    parser.add_argument('--predict-migraine', action='store_true', help='Run migraine prediction')
    parser.add_argument('--prediction-data', type=str, help='Path to data for prediction')
    parser.add_argument('--model-id', type=str, help='ID of the model to use for prediction')
    parser.add_argument('--save-predictions', action='store_true', help='Save prediction results')
    
    # Visualization and summary options
    parser.add_argument('--visualize', action='store_true', help='Visualize results')
    parser.add_argument('--summary', action='store_true', help='Print summary of results')
    
    return parser.parse_args()

def main():
    """
    Main entry point for the CLI.
    """
    args = parse_args()
    
    if args.optimize:
        run_optimization(args)
    
    if args.evaluate:
        run_evaluation(args)
    
    if args.meta:
        run_meta_learning(args)
    
    if args.drift:
        run_drift_detection(args)
    
    if args.explain:
        run_explainability_analysis(args)
    
    if args.run_meta_learner_with_drift:
        run_meta_learner_with_drift_detection(args)
    
    if args.explain_drift:
        explain_drift(args)
        
    if args.import_migraine_data:
        import_results = run_migraine_data_import(args)
        if not import_results["success"]:
            logging.error(f"Migraine data import failed: {import_results.get('error', 'Unknown error')}")
        
    if args.predict_migraine:
        prediction_results = run_migraine_prediction(args)
        if not prediction_results["success"]:
            logging.error(f"Migraine prediction failed: {prediction_results.get('error', 'Unknown error')}")

    print("All operations completed successfully.")

if __name__ == "__main__":
    main()

def run_migraine_data_import(args):
    """
    Import new migraine data with potentially different schema.
    
    Args:
        args: Command-line arguments containing data path, output path, and options
    
    Returns:
        Dictionary with results of the import operation
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Importing migraine data from {args.data_path}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Import the data
        imported_data = predictor.import_data(
            data_path=args.data_path,
            file_format=args.file_format,
            add_new_columns=args.add_new_columns
        )
        
        # If requested, add derived features
        if args.derived_features:
            for feature_def in args.derived_features:
                name, formula = feature_def.split(":", 1)
                predictor.add_derived_feature(name=name, formula=formula)
                logging.info(f"Added derived feature: {name} with formula: {formula}")
        
        # If requested, train a model with the imported data
        if args.train_model:
            model_id = predictor.train(
                data=imported_data,
                model_name=args.model_name or "migraine_model",
                description=args.model_description or f"Model trained with data from {args.data_path}",
                make_default=args.make_default
            )
            logging.info(f"Trained model with ID: {model_id}")
            model_info = {"model_id": model_id}
        else:
            model_info = {}
        
        # Save the imported data if requested
        if args.save_processed_data:
            output_path = os.path.join(args.data_dir, "processed_data.csv")
            imported_data.to_csv(output_path, index=False)
            logging.info(f"Saved processed data to {output_path}")
        
        # Get schema information
        schema_info = predictor.get_schema_info()
        
        # If summary is requested, print it
        if args.summary:
            print("\nMigraine Data Import Summary:")
            print(f"Imported data shape: {imported_data.shape}")
            print(f"New columns added: {len(schema_info['optional_features'])}")
            print(f"Derived features created: {len(schema_info['derived_features'])}")
            if args.train_model:
                print(f"Model trained with ID: {model_id}")
            print(f"Feature columns: {predictor.feature_columns}")
        
        return {
            "success": True,
            "data_shape": imported_data.shape,
            "schema_info": schema_info,
            "model_info": model_info
        }
        
    except Exception as e:
        logging.error(f"Error importing migraine data: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

def run_migraine_prediction(args):
    """
    Run prediction using a migraine model on new data, handling missing features.
    
    Args:
        args: Command-line arguments containing prediction parameters
        
    Returns:
        Dictionary with prediction results
    """
    if not MIGRAINE_MODULES_AVAILABLE:
        logging.error("Migraine prediction modules are not available. Please install the package first.")
        return {"success": False, "error": "Migraine modules not available"}
    
    try:
        logging.info(f"Running migraine prediction using data from {args.prediction_data}")
        
        # Initialize the predictor
        predictor = MigrainePredictorV2(
            model_dir=args.model_dir,
            data_dir=args.data_dir
        )
        
        # Load the model if specified
        if args.model_id:
            predictor.load_model(args.model_id)
            logging.info(f"Loaded model with ID: {args.model_id}")
        
        # Import prediction data
        pred_data = predictor.import_data(
            data_path=args.prediction_data,
            add_new_columns=False  # Don't add new columns for prediction
        )
        
        # Make predictions with missing features
        predictions = predictor.predict_with_missing_features(pred_data)
        
        # Format results
        results = []
        for i, pred in enumerate(predictions):
            sample_result = {
                "index": i,
                "prediction": pred["prediction"],
                "probability": pred["probability"],
                "top_features": sorted(
                    pred["feature_importances"].items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5] if "feature_importances" in pred else []
            }
            results.append(sample_result)
        
        # Save results if requested
        if args.save_predictions:
            # Create DataFrame with predictions
            pred_df = pd.DataFrame({
                "prediction": [p["prediction"] for p in predictions],
                "probability": [p["probability"] for p in predictions]
            })
            # Add original data
            pred_df = pd.concat([pred_data.reset_index(drop=True), pred_df], axis=1)
            output_path = os.path.join(args.data_dir, "predictions.csv")
            pred_df.to_csv(output_path, index=False)
            logging.info(f"Saved predictions to {output_path}")
        
        # If summary is requested, print it
        if args.summary:
            print("\nMigraine Prediction Summary:")
            print(f"Number of samples predicted: {len(predictions)}")
            migraine_count = sum(1 for p in predictions if p["prediction"] == 1)
            print(f"Predicted migraines: {migraine_count} ({migraine_count/len(predictions)*100:.2f}%)")
            print(f"Average probability: {sum(p['probability'] for p in predictions)/len(predictions):.4f}")
            print("\nSample predictions:")
            for i, result in enumerate(results[:5]):  # Show first 5 predictions
                print(f"Sample {i}: {'Migraine' if result['prediction'] == 1 else 'No Migraine'} " +
                      f"(Probability: {result['probability']:.4f})")
        
        return {
            "success": True,
            "predictions": results,
            "summary": {
                "total_samples": len(predictions),
                "predicted_migraines": sum(1 for p in predictions if p["prediction"] == 1),
                "average_probability": sum(p["probability"] for p in predictions)/len(predictions)
            }
        }
        
    except Exception as e:
        logging.error(f"Error running migraine prediction: {str(e)}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}
